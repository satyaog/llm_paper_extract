{
  "paper": "2312.14279.txt",
  "words": 16269,
  "extractions": {
    "description": "This paper proposes an intention detection framework to classify developer forum posts by their intentions (such as asking for help, sharing information, etc.) using a transformer-based pre-trained model. The proposed model significantly outperforms existing state-of-the-art methods in classifying post intentions.",
    "title": {
      "value": "Characterizing and Classifying Developer Forum Posts with their Intentions",
      "justification": "This title accurately reflects the core subject and purpose of the paper, which is to classify developer forum posts according to their intentions.",
      "quote": "Characterizing and Classifying Developer Forum Posts with their Intentions"
    },
    "type": {
      "value": "empirical",
      "justification": "The paper describes an empirical study involving the collection and manual analysis of a dataset from developer forums, as well as the development and evaluation of a transformer-based model for intention classification.",
      "quote": "...we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions. ... we propose an intention prediction framework for technical online posts."
    },
    "research_field": {
      "value": "Natural Language Processing (NLP)",
      "justification": "The paper focuses on classifying posts from online forums using transformer-based language models, a prominent method in NLP.",
      "quote": "In the framework, we employ transformer-based pre-trained language models to generate embeddings for both title and description of posts."
    },
    "sub_research_field": {
      "value": "Text Classification",
      "justification": "The core task in the paper is to classify developer forum posts according to their intentions, which falls under the category of text classification.",
      "quote": "Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.). Furthermore, we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions. Based on the findings and insights from the qualitative study, we propose an intention prediction framework for technical online posts."
    },
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is one of the pre-trained language models used in the proposed framework.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs... We use the pooler output of the PTMs, which corresponds to the representation of the first token... As by fine-tuning this layer with our task, the quality of embedding may be improved for our downstream task."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "BERT is a widely used transformer-based pre-trained language model in NLP tasks.",
          "quote": "...The original BERT is released in two sizes. We use the BERTbase in the experiment. The BERTbase has 12 layers of transformer block with a hidden unit size of 768 and 12 self-attention heads in the encoder stack. In total, it contains 110M parameters and is trained with a large corpus of English data."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa, another pre-trained language model, is evaluated in the proposed framework.",
          "quote": "RoBERTa (Liu et al., 2019) modified some hyper-parameters and training tasks while maintaining the original BERT architecture."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "RoBERTa is an improved variant of BERT, another transformer-based pre-trained language model.",
          "quote": "RoBERTa (Liu et al., 2019) modified some hyper-parameters and training tasks while maintaining the original BERT architecture."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "ALBERT is another variant of BERT utilized in the study.",
          "quote": "ALBERT (Lan et al., 2019) further improve the original BERT by adopting parameter reduction techniques."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "ALBERT is a smaller, faster version of BERT, designed for efficient NLP tasks.",
          "quote": "ALBERT (Lan et al., 2019) further improve the original BERT by adopting parameter reduction techniques."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is used as a more lightweight version of BERT in the framework.",
          "quote": "DistilBERT (Sanh et al., 2019) is a distilled version, which has 40% fewer parameters while maintaining over 95% of the BERT model."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "DistilBERT is a lightweight version of BERT, designed to perform well with fewer parameters.",
          "quote": "DistilBERT (Sanh et al., 2019) is a distilled version, which has 40% fewer parameters while maintaining over 95% of the BERT model."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "BERTOverflow",
          "justification": "BERTOverflow is used for its domain-specific training on Stack Overflow data.",
          "quote": "BERTOverflow (Tabassum et al., 2020) is proposed with a named entity recognition technique. It is trained with sentences from Stack Overflow and can achieve better performance on domain-specific tasks."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "BERTOverflow is a domain-specific variant of BERT focused on programming-related texts.",
          "quote": "BERTOverflow (Tabassum et al., 2020) is proposed with a named entity recognition technique. It is trained with sentences from Stack Overflow and can achieve better performance on domain-specific tasks."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is employed for its adaptation to both programming languages and natural languages.",
          "quote": "Pre-trained with both natural language corpus and programming language data, CodeBERT (Feng et al., 2020) is able to generate embeddings for both forms of input data."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based Pretrained Language Model",
          "justification": "CodeBERT is designed to handle both natural language and code, making it suitable for software engineering tasks.",
          "quote": "Pre-trained with both natural language corpus and programming language data, CodeBERT (Feng et al., 2020) is able to generate embeddings for both forms of input data."
        },
        "mode": "fine-tuned"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Stack Overflow",
          "justification": "The dataset includes posts from Stack Overflow, a major source of technical discussions among developers.",
          "quote": "In our study, our primary goal is to narrow the gap by integrating industry insights into the construction of an intention detection approach for technical forum posts. ... Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.)."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Discourse forums",
          "justification": "The dataset also includes posts from Discourse forums, contributing to the diversity of the technical communities covered.",
          "quote": "Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.). Furthermore, we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Lithium forums",
          "justification": "Lithium forums are another source of developer posts included in the dataset.",
          "quote": "The dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3 , Lithium4 forums and Discourse5 forums."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face",
          "justification": "The Hugging Face library provided the transformer-based pre-trained models used in the study.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above. We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1673,
    "prompt_tokens": 23862,
    "total_tokens": 25535
  }
}