{
  "paper": "2307.08863.txt",
  "words": 9343,
  "extractions": {
    "description": "The paper proposes Meta-Value Learning (MeVa), a consistent and far-sighted approach for learning in multi-agent systems. It improves upon previous methods by using the gradient of a meta-value function and accounting for future optimization iterates.",
    "title": {
      "value": "Meta-Value Learning: A General Framework for Learning with Learning Awareness",
      "justification": "The title succinctly reflects the main contribution and scope of the paper.",
      "quote": "META -VALUE L EARNING : A G ENERAL F RAMEWORK FOR L EARNING WITH L EARNING AWARENESS"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper evaluates the proposed MeVa method using experiments on several environments and compares its performance with other existing methods.",
      "quote": "We analyze the behavior of our method on a toy game and compare to prior work on repeated matrix games."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper deals with deep learning techniques in the context of multi-agent systems and reinforcement learning.",
      "quote": "We study P -player differentiable games f : RP ×N 7→ RP that map vectors of policies xi to vectors of expected returns yi"
    },
    "sub_research_field": {
      "value": "Multi-Agent Reinforcement Learning",
      "justification": "The paper specifically discusses learning in multi-agent systems and the interactions between agents.",
      "quote": "Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents’ learning processes."
    },
    "models": [
      {
        "name": {
          "value": "Meta-Value Learning (MeVa)",
          "justification": "MeVa is the proposed model and main contribution of the paper.",
          "quote": "The resulting method, MeVa, is consistent and far-sighted."
        },
        "role": "Contributed",
        "type": {
          "value": "Reinforcement Learning Model",
          "justification": "MeVa applies reinforcement learning techniques to the meta-game of optimization.",
          "quote": "We apply a form of Q-learning to the meta-game of optimization, in a way that avoids the need to explicitly represent the continuous action space of policy updates."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "Naive Learning",
          "justification": "Naive Learning is used as a baseline for comparison among other methods.",
          "quote": "The naive application of gradient descent (see §2.1) fails to find tit-for-tat on the IPD unless initialized sufficiently close to it."
        },
        "role": "Used",
        "type": {
          "value": "Reinforcement Learning Model",
          "justification": "It is a basic model applying standard gradient descent in the context of reinforcement learning.",
          "quote": "Naive learning is the straightforward application of standard gradient descent which is popular when optimizing a single objective."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "LOLA (Learning with Opponent-Learning Awareness)",
          "justification": "LOLA is used as a comparison point to demonstrate the improvements made by MeVa.",
          "quote": "We take inspiration from the recent work Learning with Opponent-Learning Awareness (LOLA Foerster et al. (2018a;c)), the first general learning algorithm to find tit-for-tat on IPD."
        },
        "role": "Referenced",
        "type": {
          "value": "Reinforcement Learning Model",
          "justification": "LOLA is a model specifically designed for multi-agent reinforcement learning contexts.",
          "quote": "LOLA (Foerster et al., 2018a) accounts for this by differentiating through one step of optimization."
        },
        "mode": "Trained"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Iterated Prisoner's Dilemma (IPD)",
          "justification": "IPD is used to evaluate the effectiveness of the proposed MeVa method.",
          "quote": "In a tournament on repeated matrix games (§5.2), MeVa exhibits opponent-shaping behavior, including ZD-extortion (Press & Dyson, 2012) on the IPD."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Iterated Matching Pennies (IMP)",
          "justification": "IMP is also used to evaluate the MeVa method, focusing on its ability to handle different game dynamics.",
          "quote": "On Iterated Matching Pennies (Table 2b), MeVa exploits naive and LOLA learners, moreso than M-FOS."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Chicken Game",
          "justification": "The Chicken Game is another environment used in the paper to test the performance of MeVa.",
          "quote": "On the Chicken Game (Table 2c), LOLA exploits every opponent except M-MAML, but does poorly against itself (also observed by Lu et al. (2022))."
        },
        "role": "Used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is used for scientific computing and model implementation in the paper.",
          "quote": "We used the JAX (Bradbury et al., 2018) library for scientific computing."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "NumPy",
          "justification": "NumPy is mentioned as a part of the computational setup in the paper.",
          "quote": "JAX: composable transformations of Python+NumPy programs"
        },
        "role": "Referenced"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1286,
    "prompt_tokens": 15812,
    "total_tokens": 17098
  }
}