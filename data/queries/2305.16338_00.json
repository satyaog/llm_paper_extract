{
  "paper": "2305.16338.txt",
  "words": 9242,
  "extractions": {
    "description": "The paper proposes a novel architecture, Decision Transformers with Memory (DT-Mem), which introduces an internal working memory module to improve training efficiency and generalization for large language model-based decision-making agents. The working memory module helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Evaluation results demonstrate that DT-Mem improves performance in Atari games and Meta-World object manipulation tasks while reducing the number of parameters and training time required.",
    "title": {
      "value": "Think Before You Act: Decision Transformers with Internal Working Memory",
      "justification": "This value is the title of the paper as mentioned in the provided text.",
      "quote": "Think Before You Act: Decision Transformers with Internal Working Memory"
    },
    "type": {
      "value": "empirical",
      "justification": "The paper proposes a new model (DT-Mem) and evaluates its performance experimentally on Atari games and Meta-World environments, showing improved generalization and efficiency.",
      "quote": "Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The research focuses on improving the efficiency and generalization of large language model-based decision-making agents, which falls under the umbrella of Deep Learning.",
      "quote": "Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks."
    },
    "sub_research_field": {
      "value": "Reinforcement Learning",
      "justification": "The paper's proposed model, DT-Mem, is applied to reinforcement learning settings, including Atari games and Meta-World environments, addressing issues specific to RL such as generalization and adaptability.",
      "quote": "Recently, with the tremendous success of large language model-based (LLM-based) foundation models [5, 27, 12, 33], an increasing number of researchers have focused on LLM-based decision-making agents."
    },
    "models": [
      {
        "name": {
          "value": "DT-Mem",
          "justification": "The main contribution of the paper is the Decision Transformers with Memory (DT-Mem), which is introduced as a new model to enhance training efficiency and generalization.",
          "quote": "Thus motivated, we propose Decision Transformers with Memory (DT-Mem)."
        },
        "role": "contributed",
        "type": {
          "value": "Transformer-based model with internal working memory",
          "justification": "DT-Mem is a Transformer-based model that includes an internal working memory module to store, blend, and retrieve information for different downstream tasks.",
          "quote": "We propose Decision Transformers with Memory (DT-Mem), a novel Transformer-based DT that improves model generalization, computational efficiency and model efficiency."
        },
        "mode": "trained"
      },
      {
        "name": {
          "value": "Multi-game DT (MDT)",
          "justification": "Multi-game Decision Transformer (MDT) is referenced in the paper as a pre-trained model that DT-Mem relies on for its generalization capacity.",
          "quote": "For example, in Multi-game DT [22] and Hyper-DT [38], and this pre-training enables them to capture broad knowledge that is transferable across tasks."
        },
        "role": "referenced",
        "type": {
          "value": "Transformer-based model",
          "justification": "Multi-game DT is described as a Transformer-based model used for generalization across various tasks.",
          "quote": "For example, in Multi-game DT [22] and Hyper-DT [38], and this pre-training enables them to capture broad knowledge that is transferable across tasks."
        },
        "mode": "pre-trained"
      },
      {
        "name": {
          "value": "Hyper-DT (HDT)",
          "justification": "Hyper-Decision Transformer (HDT) is another pre-trained model referenced in the paper to highlight the generalization capabilities of Transformer-based models.",
          "quote": "For example, in Multi-game DT [22] and Hyper-DT [38], and this pre-training enables them to capture broad knowledge that is transferable across tasks."
        },
        "role": "referenced",
        "type": {
          "value": "Transformer-based model",
          "justification": "Hyper-DT is described as a Transformer-based model that enhances generalization and adaptability across tasks.",
          "quote": "For example, in Multi-game DT [22] and Hyper-DT [38], and this pre-training enables them to capture broad knowledge that is transferable across tasks."
        },
        "mode": "pre-trained"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari Games",
          "justification": "The paper evaluates DT-Mem using Atari games to demonstrate its improved training efficiency and generalization capabilities.",
          "quote": "Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Meta-World",
          "justification": "The Meta-World environment is used to evaluate DT-Mem's performance in object manipulation tasks, showing its adaptability and generalization.",
          "quote": "To validate our approach, we evaluate DT-Mem on Atari games, as used in Multi-game Decision Transformer (MDT) [22], and Meta-World environments, as used in Prompt Decision Transformer (PDT) [37] and Hyper-Decision Transformer (HDT) [38]."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LoRA",
          "justification": "The LoRA (Low-Rank Adaptation) method is used in conjunction with the memory module to modulate its output and enhance adaptability to new tasks.",
          "quote": "In particular, we use the low-rank adaptation (LoRA) [18] method in conjunction with a small set of adaptation parameters to modulate the memory moduleâ€™s output."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1089,
    "prompt_tokens": 15032,
    "total_tokens": 16121
  }
}