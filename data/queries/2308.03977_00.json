{
  "paper": "2308.03977.txt",
  "words": 17241,
  "extractions": {
    "description": "The paper introduces Photorealistic Unreal Graphics (PUG) environments and datasets for representation learning. The datasets, generated using Unreal Engine, aim to bridge the gap between synthetic and real-world data by offering controllability and realism. The work includes the introduction of several new datasets aimed at different aspects of representation learning, generalization, and robustness testing.",
    "title": {
      "value": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning",
      "justification": "This is the title provided at the beginning of the paper.",
      "quote": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"
    },
    "type": {
      "value": "empirical study",
      "justification": "The paper involves the creation of datasets and environments, and empirical testing of various vision models using these datasets.",
      "quote": "We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models."
    },
    "research_field": {
      "value": "Computer Vision",
      "justification": "The focus of the paper is on creating and evaluating datasets and models for visual tasks.",
      "quote": "We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning."
    },
    "sub_research_field": {
      "value": "Representation Learning",
      "justification": "The datasets and environments are designed to improve representation learning by providing controllable and realistic synthetic data.",
      "quote": "We introduce the Photorealistic Unreal Graphics (PUG) environments, a family of 3D graphics environments that leverage Unreal Engine for rendering image data for representation learning research."
    },
    "models": [
      {
        "name": {
          "value": "ResNet50",
          "justification": "The model is used to test the generalization capabilities in various scenarios.",
          "quote": "In Figure 3, we present our results training a ResNet50 with different held out factors."
        },
        "role": "used",
        "type": {
          "value": "CNN",
          "justification": "ResNet50 is a Convolutional Neural Network (CNN) widely used for image classification tasks.",
          "quote": "In Figure 3, we present our results training a ResNet50 with different held out factors."
        },
        "mode": "trained"
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "Several variants of CLIP are tested for vision-language tasks.",
          "quote": "For example, the hard negative caption of 'An elephant on the left of the picture and a camel on the right of the picture' will be 'A camel on the left of the picture and an elephant on the right of the picture'."
        },
        "role": "used",
        "type": {
          "value": "Vision-Language Model",
          "justification": "CLIP is a recently introduced model for vision-language tasks, which aligns images and text in a joint embedding space.",
          "quote": "In Figure 6, we present the performances of several VLMs with hard negatives captioning on PUG: SPAR in which we perform retrieval between two captions: the correct caption and the hard-negative corresponding caption."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "DINOv2",
          "justification": "DINOv2 is evaluated for robustness on various factors in the PUG: ImageNet dataset.",
          "quote": "We assess a variety of model architectures across several pretraining datasets including ImageNet-1/-21k, LAION (400M and 2B), and JFT300M. We observe in Table 1 that the models that perform the best on the ImageNet validation accuracy are not always the ones which offer the best robustness on PUG: ImageNet."
        },
        "role": "used",
        "type": {
          "value": "Self-Supervised Learning Model",
          "justification": "DINOv2 is a self-supervised learning model aimed at learning robust visual features.",
          "quote": "The recent self-supervised DINOv2 model [...] We observe in Table 1 that the models that perform the best on the ImageNet validation accuracy are not always the ones which offer the best robustness on PUG: ImageNet."
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "PUG: Animals",
          "justification": "PUG: Animals is a dataset introduced by the authors for research on out-of-distribution generalization and studying representational space.",
          "quote": "We present PUG: Animals, a new photorealistic synthetic dataset with annotated factors of variations to evaluate the out-of-distribution (OOD) robustness of models."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "PUG: ImageNet",
          "justification": "PUG: ImageNet is another dataset introduced in the paper for fine-grained robustness evaluation.",
          "quote": "We introduce PUG: ImageNet as an additional robustness test set to ImageNet, containing a rich set of factor changes such as pose, background, size, texture, and lighting."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "PUG: SPAR",
          "justification": "PUG: SPAR is designed to evaluate vision-language models.",
          "quote": "As a third member of the PUG family, we introduce PUG: SPAR (Scene, Position, Attribute and Relation) for evaluating vision-language models (VLMs)."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "PUG: AR4T",
          "justification": "PUG: AR4T is introduced for fine-tuning vision-language models and demonstrating the reliability of PUG: SPAR.",
          "quote": "In addition, we introduce PUG: AR4T for fine-tuning vision-language models and use it to demonstrate the reliability of PUG: SPAR in contrast to other benchmarks."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as a benchmark to compare the robustness of models against PUG: ImageNet.",
          "quote": "The PUG: ImageNet dataset offers both photo-realism and precise control over how each object is depicted from pose and size to environment and camera-angle. We also provide a collection of objects with mappings to classes in the popular ImageNet dataset."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TorchMultiverse",
          "justification": "TorchMultiverse is a Python library introduced in the paper to interact with the PUG environments.",
          "quote": "we also introduce the TorchMultiverse python library, which offers a simple python interface to enable easily controlled dataset creation from any given PUG environment."
        },
        "role": "contributed"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1603,
    "prompt_tokens": 31167,
    "total_tokens": 32770
  }
}