{
  "paper": "2312.14279.txt",
  "words": 16269,
  "extractions": {
    "description": "The paper focuses on the characterization and classification of developer forum posts based on their intentions. The authors propose a refined taxonomy for the intentions of technical forum posts and design a pre-trained transformer-based model to predict these intentions automatically. Their best model variant achieves competitive performance.",
    "title": {
      "value": "Characterizing and Classifying Developer Forum Posts with their Intentions",
      "justification": "The title is taken directly from the research paper to ensure accuracy.",
      "quote": "Characterizing and Classifying Developer Forum Posts with their Intentions"
    },
    "type": {
      "value": "Empirical study",
      "justification": "The paper involves empirical methods such as qualitative studies and experimental evaluations to analyze the data and validate the proposed model.",
      "quote": "In this paper, we refer to these reasons as intentions. To exemplify the distinctions between technique-oriented tag taxonomies and an intention-based taxonomy for technical posts, we present a concrete example."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The study utilizes deep learning models to predict the intentions of forum posts, making it a part of the Deep Learning research field.",
      "quote": "Through manual labeling and analysis on a sampled post dataset extracted from online forums, we understand the relevance between the constitution of posts (code, error messages) and their intentions."
    },
    "sub_research_field": {
      "value": "Natural Language Processing",
      "justification": "The paper uses transformer-based models, which are a key technology in Natural Language Processing, to analyze and predict the intentions of forum posts.",
      "quote": "In the framework, we employ transformer-based pre-trained language models to generate embeddings for both title and description of posts."
    },
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including BERT."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "BERT is a well-known transformer-based model used for various NLP tasks.",
          "quote": "BERT: Bidirectional Encoder Representations from Transformers."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including RoBERTa."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "RoBERTa is a variant of BERT that modifies the pre-training process to improve performance.",
          "quote": "RoBERTa: A Robustly Optimized BERT Pretraining Approach."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "ALBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including ALBERT."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "ALBERT is a variant of BERT that uses parameter-sharing techniques to reduce model complexity.",
          "quote": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including DistilBERT."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "DistilBERT is a smaller, faster version of BERT that retains most of its performance.",
          "quote": "DistilBERT: A Distilled Version of BERT."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "BERTOverflow",
          "justification": "BERTOverflow is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including BERTOverflow."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "BERTOverflow is a variant of BERT trained on data from Stack Overflow to better understand software engineering-related text.",
          "quote": "BERTOverflow: Trained with Sentences from Stack Overflow."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including CodeBERT."
        },
        "role": "used",
        "type": {
          "value": "Transformer-based model",
          "justification": "CodeBERT is specialized for understanding both natural language and programming language.",
          "quote": "CodeBERT: A Pre-trained Model for Programming and Natural Languages."
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Stack Overflow",
          "justification": "Stack Overflow posts are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Discourse forums",
          "justification": "Discourse forums are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Lithium forums",
          "justification": "Lithium forums are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face",
          "justification": "Hugging Face's implementation of transformer models is used in the study.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1269,
    "prompt_tokens": 23932,
    "total_tokens": 25201
  }
}