{
  "paper": "2312.11669.txt",
  "words": 22802,
  "extractions": {
    "description": "This paper proposes a novel approach for value function estimation in continual reinforcement learning by decomposing the value function into two components: a permanent value function, and a transient value function. This decomposition allows for better handling of the stability-plasticity dilemma, improving the agent's ability to retain long-term knowledge while quickly adapting to new information.",
    "title": {
      "value": "Prediction and Control in Continual Reinforcement Learning",
      "justification": "The title directly reflects the main focus of the paper.",
      "quote": "Prediction and Control in Continual Reinforcement Learning"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes empirical case studies and experiments validating the proposed methods' effectiveness.",
      "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper addresses methodologies in reinforcement learning, which is a subset of deep learning.",
      "quote": "Deep reinforcement learning (RL) has achieved remarkable successes in complex tasks, e.g., Go"
    },
    "sub_research_field": {
      "value": "Continual Reinforcement Learning",
      "justification": "The paper specifically focuses on continual learning in reinforcement learning environments.",
      "quote": "Let S be the set of possible states and A the set of actions. At each timestep t, the agent takes action At ∈ A in state St according to its (stochastic) policy π... This quantity can be estimated with a function approximator parameterized by w, for example using TD learning."
    },
    "models": [
      {
        "name": {
          "value": "PT-TD learning",
          "justification": "The paper introduces PT-TD learning as a part of their continual reinforcement learning approach.",
          "quote": "PT-TD learning (Prediction)"
        },
        "role": "contributed",
        "type": {
          "value": "Reinforcement Learning Model",
          "justification": "PT-TD learning is designed as a reinforcement learning method specifically for prediction tasks.",
          "quote": "Our contributions are: RL algorithms for prediction and control using permanent and transient value functions."
        },
        "mode": "trained"
      },
      {
        "name": {
          "value": "PT-Q-learning",
          "justification": "The paper introduces PT-Q-learning as a part of their continual reinforcement learning approach.",
          "quote": "PT-Q-learning (Control)"
        },
        "role": "contributed",
        "type": {
          "value": "Reinforcement Learning Model",
          "justification": "PT-Q-learning is designed as a reinforcement learning method specifically for control tasks.",
          "quote": "Our contributions are: RL algorithms for prediction and control using permanent and transient value functions."
        },
        "mode": "trained"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Minigrid",
          "justification": "Minigrid was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11]"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "JellyBeanWorld (JBW)",
          "justification": "JBW was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31]"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "MinAtar",
          "justification": "MinAtar was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SGD optimizer",
          "justification": "SGD optimizer is mentioned as one of the optimization methods used in the paper.",
          "quote": "Our approach performs fast and slow interplay at various levels...The transient value function is updated using a larger learning rate (fast) [3]."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Adam optimizer",
          "justification": "Adam optimizer is mentioned as one of the optimization methods used in the paper.",
          "quote": "All the results are reported using Adam optimizer to update weights."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 837,
    "prompt_tokens": 43093,
    "total_tokens": 43930
  }
}