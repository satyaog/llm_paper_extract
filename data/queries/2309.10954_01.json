{
  "paper": "2309.10954.txt",
  "words": 7266,
  "extractions": {
    "title": {
      "value": "In-Context Learning for Text Classification with Many Labels",
      "justification": "The title clearly reflects the content and focus of the paper, which is on leveraging in-context learning for text classification tasks involving numerous labels.",
      "quote": "In-Context Learning for Text Classification with Many Labels"
    },
    "description": "The paper studies in-context learning (ICL) using large language models (LLMs) for tasks with many labels, addressing the challenge posed by the limited context window of these models. It proposes a method to overcome this limitation by using a pre-trained dense retrieval model to provide the model with a partial view of the full label space for each inference call. The method is evaluated on multiple datasets, showing state-of-the-art performance in certain settings without finetuning.",
    "type": {
      "value": "empirical",
      "justification": "The study involves empirical evaluation of methods using experiments on multiple datasets and models to demonstrate performance outcomes.",
      "quote": "By testing on intent classification (upwards of 50 classes) and fine-grained sentiment analysis (upwards of 25 classes), we demonstrate that the resulting performance with this method can reach SoTA."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper discusses methods and experiments involving large language models and deep learning techniques.",
      "quote": "In-context learning (ICL) using large language models (LLMs) has recently exploded in popularity."
    },
    "sub_research_fields": [
      {
        "value": "Natural Language Processing (NLP)",
        "justification": "The study specifically focuses on text classification tasks, which is a key area within NLP.",
        "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
      }
    ],
    "models": [
      {
        "name": {
          "value": "OPT",
          "justification": "The paper mentions testing with OPT models to achieve state-of-the-art performance in few-shot settings.",
          "quote": "Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings."
        },
        "caracteristics": [
          {
            "value": "Large Language Model (LLM)",
            "justification": "OPT is categorized as a large language model trained for various NLP tasks.",
            "quote": "Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "The paper mentions testing with LLaMA models to achieve state-of-the-art performance in few-shot settings.",
          "quote": "Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings."
        },
        "caracteristics": [
          {
            "value": "Large Language Model (LLM)",
            "justification": "LLaMA is categorized as a large language model trained for various NLP tasks.",
            "quote": "Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DeBERTa-v2-XXLarge",
          "justification": "The paper compares the performance of their method with the DeBERTa-v2-XXLarge model.",
          "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the 'Pfeiffer' bottleneck-style adapter...)."
        },
        "caracteristics": [
          {
            "value": "Large Language Model (LLM)",
            "justification": "DeBERTa-v2-XXLarge is categorized as a large language model pre-trained for various NLP tasks.",
            "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the 'Pfeiffer' bottleneck-style adapter...)."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SBERT",
          "justification": "The paper uses a pre-trained SBERT model as a retriever for in-context learning.",
          "quote": "By coupling the LLM with an external pre-trained dense retriever model (Reimers and Gurevych, 2019a; Karpukhin et al., 2020), we can dynamically retrieve a set of examples to provide to the LM in-context."
        },
        "caracteristics": [
          {
            "value": "Sentence Transformer",
            "justification": "SBERT is a sentence transformer model designed for sentence embeddings and retrieval tasks.",
            "quote": "By coupling the LLM with an external pre-trained dense retriever model (Reimers and Gurevych, 2019a; Karpukhin et al., 2020), we can dynamically retrieve a set of examples to provide to the LM in-context."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BANKING77",
          "justification": "The paper evaluates its methods using the BANKING77 dataset.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "HWU64",
          "justification": "The paper evaluates its methods using the HWU64 dataset.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CLINC150",
          "justification": "The paper evaluates its methods using the CLINC150 dataset.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GoEmotions",
          "justification": "The paper evaluates its methods using the GoEmotions dataset for fine-grained sentiment classification.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AdapterHub",
          "justification": "AdapterHub is referenced as a tool used for implementing fine-tuning methods in the paper.",
          "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the 'Pfeiffer' bottleneck-style adapter (Pfeiffer et al., 2020b) implemented with AdapterHub..."
        },
        "role": "referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SentenceTransformers",
          "justification": "The paper uses the SentenceTransformers library for the SBERT retrieval model.",
          "quote": "For our sentence encoder/retriever, we use the SentenceTransformers library (Reimers and Gurevych, 2019a)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SetFit",
          "justification": "The paper compares its method's performance to SetFit, a lightweight method for contrastive training.",
          "quote": "SetFit (Tunstall et al., 2022) results are also provided, a method involving contrastive fine-tuning of a retriever model with a classification head, as it is also a competitive and lightweight baseline in this setup."
        },
        "role": "referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1509,
    "prompt_tokens": 12898,
    "total_tokens": 14407
  }
}