{
  "paper": "2306.14808.txt",
  "words": 13970,
  "extractions": {
    "description": "The paper presents ηψ-Learning, an algorithm designed to improve exploration efficiency within reinforcement learning by combining predecessor and successor representations to maximize the state visitation distribution entropy of a single finite-length trajectory.",
    "title": {
      "value": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
      "justification": "This is the exact title as presented in the document.",
      "quote": "Maximum State Entropy Exploration using Predecessor and Successor Representations"
    },
    "type": {
      "value": "theoretical",
      "justification": "The paper focuses on the proposal and evaluation of a new algorithm (ηψ-Learning) for reinforcement learning, emphasizing theoretical contributions such as combining predecessor and successor representations.",
      "quote": "In this work, we present ηψ-Learning, an algorithm to learn an exploration policy that methodically searches through a task."
    },
    "research_field": {
      "value": "deep learning",
      "justification": "The research is focused on advancing techniques within reinforcement learning, a subfield of deep learning, through the development of a novel exploration algorithm.",
      "quote": "This study of developing exploration behaviors in reinforcement learning is guided by a fundamental curiosity about the nature of autonomous learning."
    },
    "sub_research_field": {
      "value": "reinforcement learning",
      "justification": "The paper explicitly deals with solving problems in reinforcement learning by introducing ηψ-Learning.",
      "quote": "The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent’s environment via intrinsic motivation to accelerate learning optimal policies."
    },
    "models": [
      {
        "name": {
          "value": "ηψ-Learning",
          "justification": "ηψ-Learning is the main model introduced and evaluated in this study to enhance exploration efficiency.",
          "quote": "In this work, we propose ηψ-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory"
        },
        "role": "contributed",
        "type": {
          "value": "exploration algorithm",
          "justification": "The model is designed to improve exploration strategies within reinforcement learning.",
          "quote": "ηψ-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory."
        },
        "mode": "trained"
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "RLHive",
          "justification": "RLHive was used to implement the proposed method ηψ-Learning and to conduct the experiments.",
          "quote": "The implementation of the proposed method was done using the RLHive (Patil et al., 2023) library."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 600,
    "prompt_tokens": 21997,
    "total_tokens": 22597
  }
}