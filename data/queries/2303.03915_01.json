{
  "paper": "2303.03915.txt",
  "words": 16866,
  "extractions": {
    "description": "This paper documents the efforts by the BigScience workshop to create the ROOTS corpus, a massive multilingual dataset used for training the BLOOM language model. The dataset spans 59 languages, and the paper discusses methods for data collection, processing, and quality improvement, as well as ethical considerations.",
    "title": {
      "value": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
      "justification": "The title is mentioned at the beginning of the paper and aligns with the details provided about the ROOTS corpus.",
      "quote": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves the collection, curation, and analysis of a large dataset and discusses empirical results of those processes.",
      "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset."
    },
    "research_field": {
      "value": "Natural Language Processing",
      "justification": "The paper discusses the creation of a large-scale multilingual dataset used for training language models, which is a core topic in Natural Language Processing.",
      "quote": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings."
    },
    "sub_research_field": {
      "value": "Multilingual Text Corpus Creation",
      "justification": "The specific focus of the paper is on assembling a multilingual corpus, as well as discussing tools and methods for processing such data.",
      "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
    },
    "models": [
      {
        "name": {
          "value": "GPT-3",
          "justification": "The paper references GPT-3 as a comparative model to the objectives they had with BLOOM.",
          "quote": "One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in scale to GPT-3 (Brown et al., 2020)."
        },
        "role": "Referenced",
        "type": {
          "value": "Transformer-based Language Model",
          "justification": "GPT-3 is well-known as a transformer-based language model.",
          "quote": "One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in scale to GPT-3 (Brown et al., 2020)."
        },
        "mode": "Inference"
      },
      {
        "name": {
          "value": "T5",
          "justification": "The paper mentions T5 in the context of discussing the datasets used in training language models like T5.",
          "quote": "Opt: Open Pre-trained Transformer Language Models, mC4 (Raffel et al., 2020), which have powered the T5 family of models."
        },
        "role": "Referenced",
        "type": {
          "value": "Transformer-based Language Model",
          "justification": "T5 is a well-known transformer-based language model.",
          "quote": "mC4 (Raffel et al., 2020), which have powered the T5 family of models."
        },
        "mode": "Inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CC100",
          "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
          "quote": "CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling."
        },
        "role": "Referenced"
      },
      {
        "name": {
          "value": "Common Crawl",
          "justification": "The paper describes using data from the Common Crawl as part of their dataset processing.",
          "quote": "we retrieved pages corresponding to the target domain names from 18 snapshots archived by Common Crawl in 2020 and 2021 in Web ARChive (WARC) format (Mohr et al., 2008)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "GitHub",
          "justification": "The paper mentions collecting a code dataset from GitHub.",
          "quote": "We collected a code dataset from BigQuery using the same language selection as AlphaCode (Li et al., 2022)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Wikipedia",
          "justification": "The paper describes using data from Wikipedia as part of their dataset processing.",
          "quote": "We then removed entire domains whose size was less than 2MB after this step, yielding 147 pseudo-crawl-based datasets, and a total of 517 datasets including all three pipelines."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "StackExchange",
          "justification": "The paper mentions using data from StackExchange for dataset creation.",
          "quote": "we selected code data available on GitHub and StackExchange."
        },
        "role": "Used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stanza",
          "justification": "The paper mentions using Stanza for tokenization.",
          "quote": "For Arabic, Catalan, Basque, Indonesian, and Chinese (both simplified and traditional), we use the Stanza tokenizer (Qi et al., 2020)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "NLTK",
          "justification": "The paper uses NLTK for tokenization for certain languages.",
          "quote": "For English, French, Portuguese, and Spanish, we use the NLTK tokenizer (Bird et al., 2009)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Indic NLP Library",
          "justification": "The paper references the Indic NLP library for tokenization specific to certain languages.",
          "quote": "For Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Punjabi, Tamil, and Telugu, we use the Indic NLP library tokenizer (Kunchukuttan, 2020)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Underthesea",
          "justification": "The library Underthesea is used for Vietnamese tokenization in the paper.",
          "quote": "For Vietnamese, we use the Underthesea tokenizer."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "SentencePiece",
          "justification": "The paper uses SentencePiece for several tokenization and filtering tasks.",
          "quote": "Filter on perplexity score Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018) followed by KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was extracted from OSCAR."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "KenLM",
          "justification": "KenLM is used for building language models in this paper.",
          "quote": "Filter on perplexity score Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018) followed by KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was extracted from OSCAR."
        },
        "role": "Used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1365,
    "prompt_tokens": 28676,
    "total_tokens": 30041
  }
}