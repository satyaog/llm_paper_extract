{
  "paper": "2305.19452.txt",
  "words": 9099,
  "extractions": {
    "title": {
      "value": "Bigger, Better, Faster: Human-level Atari with human-level efficiency",
      "justification": "This is the explicit title of the paper provided at the beginning of the document.",
      "quote": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"
    },
    "description": "The paper introduces a value-based reinforcement learning (RL) agent called BBF, that achieves super-human performance in the Atari 100K benchmark by scaling the neural networks used for value estimation in a sample-efficient manner. It conducts extensive analyses of these design choices and provides insights for future work.",
    "type": {
      "value": "empirical study",
      "justification": "The paper conducts extensive experiments and analyses on the BBF agent's performance in the Atari 100K benchmark.",
      "quote": "We conduct extensive analyses of these design choices and provide insights for future work."
    },
    "primary_research_field": {
      "value": "Reinforcement Learning",
      "justification": "The paper focuses on developing a reinforcement learning agent and measuring its performance on the Atari 100K benchmark.",
      "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
    },
    "sub_research_fields": [
      {
        "value": "Sample-efficient Reinforcement Learning",
        "justification": "The paper aims to achieve super-human performance with human-level sample efficiency in reinforcement learning tasks.",
        "quote": "BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner."
      }
    ],
    "models": [
      {
        "name": {
          "value": "BBF",
          "justification": "BBF is the central model introduced by the paper.",
          "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "BBF is described as a value-based reinforcement learning agent designed for the Atari 100K benchmark.",
            "quote": "We introduce a value-based RL agent, which we call BBF."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DQN",
          "justification": "DQN is used as a baseline for performance comparison.",
          "quote": "...representative model-free RL methods, including DQN (Mnih et al., 2015b)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "DQN is a known reinforcement learning agent used for baseline comparisons.",
            "quote": "...representative model-free RL methods, including DQN (Mnih et al., 2015b)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Rainbow",
          "justification": "Rainbow is cited as another baseline model-free RL method for comparison.",
          "quote": "...representative model-free RL methods, including... Rainbow (Hessel et al., 2017)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "Rainbow is a known reinforcement learning agent used for baseline comparisons.",
            "quote": "...representative model-free RL methods, including... Rainbow (Hessel et al., 2017)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "IQN",
          "justification": "IQN is mentioned as an example of a model-free RL method for comparison.",
          "quote": "...representative model-free RL methods, including... IQN (Dabney et al., 2018)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "IQN is a known reinforcement learning agent used for baseline comparisons.",
            "quote": "...representative model-free RL methods, including... IQN (Dabney et al., 2018)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "EfficientZero",
          "justification": "EfficientZero is used for comparison on sample efficiency and computational costs.",
          "quote": "While human-level efficiency has been obtained by the model-based EfficientZero agent (Ye et al., 2021)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "EfficientZero is cited as a model-based RL agent used for efficiency comparison.",
            "quote": "While human-level efficiency has been obtained by the model-based EfficientZero agent (Ye et al., 2021)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "IRIS",
          "justification": "IRIS is referenced as another model-based agent for performance comparison.",
          "quote": "Micheli et al. (2023) introduce IRIS, a data-efficient agent that learns in a world model..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "IRIS is mentioned as a model-based RL agent applicable for comparison.",
            "quote": "Micheli et al. (2023) introduce IRIS, a data-efficient agent that learns in a world model..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DreamerV2",
          "justification": "DreamerV2 is mentioned as a model-based RL method for comparison.",
          "quote": "To contrast with the sample-efficiency progress in model-based RL, we also include DreamerV2 (Hafner et al., 2020)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "DreamerV2 is a known model-based reinforcement learning agent used for comparisons.",
            "quote": "To contrast with the sample-efficiency progress in model-based RL, we also include DreamerV2 (Hafner et al., 2020)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MuZero",
          "justification": "MuZero Reanalyse is cited for comparing sample-efficiency in RL.",
          "quote": "To contrast with the sample-efficiency progress in model-based RL, we also include ... MuZero Reanalyse (Schrittwieser et al., 2021)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "MuZero Reanalyse is a known model-based RL agent referenced for efficiency comparisons.",
            "quote": "To contrast with the sample-efficiency progress in model-based RL, we also include ... MuZero Reanalyse (Schrittwieser et al., 2021)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SR-SPR",
          "justification": "SR-SPR is used for benchmarking BBF's performance.",
          "quote": "BBF’s very high performance relative to some of the best-performing Atari 100K agents: EfficientZero (Ye et al., 2021), SR-SPR (D’Oro et al., 2023)..."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Agent",
            "justification": "SR-SPR is another reinforcement learning agent used in the experiments for performance comparisons.",
            "quote": "BBF’s very high performance relative to some of the best-performing Atari 100K agents: EfficientZero (Ye et al., 2021), SR-SPR (D’Oro et al., 2023)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 100K benchmark",
          "justification": "The Atari 100K benchmark is the primary dataset used for evaluating the performance of the BBF agent.",
          "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Arcade Learning Environment (ALE)",
          "justification": "ALE is used as a broader platform to discuss sample-efficient RL research in the context of the BBF agent's performance.",
          "quote": "Finally, we propose moving the goalpost for sample-efficient RL research on the ALE."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Dopamine framework",
          "justification": "The Dopamine framework is mentioned as the basis for the implementation of the BBF agent.",
          "quote": "Our implementation is based on the Dopamine framework (Castro et al., 2018)..."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "rliable",
          "justification": "The rliable library is used for evaluation metrics including interquartile mean (IQM).",
          "quote": "For evaluation, we use rliable (Agarwal et al., 2021b) and in particular, the interquartile mean (IQM) metric..."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2142,
    "prompt_tokens": 17284,
    "total_tokens": 19426
  }
}