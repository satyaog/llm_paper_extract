{
  "paper": "2307.01163.txt",
  "words": 9790,
  "extractions": {
    "description": "This paper proposes an active forgetting mechanism during the pretraining of pretrained language models (PLMs) to improve their ability to adapt quickly to new languages. By periodically resetting the embedding layer, the model enhances its ability to learn new embeddings rapidly, similar to a meta-learning effect. Experiments demonstrate faster convergence and improved performance, especially for low-resource and linguistically distant languages.",
    "title": {
      "value": "Improving Language Plasticity via Pretraining with Active Forgetting",
      "justification": "The title accurately captures the paper's focus on using active forgetting to enhance the adaptability of PLMs to new languages.",
      "quote": "Improving Language Plasticity via Pretraining with Active Forgetting"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments with RoBERTa models to validate the effectiveness of the proposed active forgetting mechanism, making it an empirical study.",
      "quote": "Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation, but also outperform standard ones in a low-data regime, particularly for languages that are distant from English."
    },
    "research_field": {
      "value": "Natural Language Processing (NLP)",
      "justification": "The paper focuses on pretrained language models and their application to cross-lingual tasks, which are key areas within NLP.",
      "quote": "Pretrained language models (PLMs) are today the primary model for natural language processing."
    },
    "sub_research_field": {
      "value": "Cross-lingual Transfer Learning",
      "justification": "The study aims to enhance the adaptability of PLMs to new languages, making it a contribution to cross-lingual transfer learning.",
      "quote": "We study whether this forgetting approach creates a PLM that can easily rewire...to an unseen (possibly distant) language."
    },
    "models": [
      {
        "name": {
          "value": "RoBERTa-base",
          "justification": "RoBERTa-base is used as the primary model for implementing and testing the proposed active forgetting mechanism.",
          "quote": "we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau et al., 2020]."
        },
        "role": "Used",
        "type": {
          "value": "Transformer-based Model",
          "justification": "The model is a transformer-based language model, commonly used in NLP tasks.",
          "quote": "we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau et al., 2020]."
        },
        "mode": "Pretraining"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CC100",
          "justification": "CC100 is used as the primary dataset for pretraining the RoBERTa-base model.",
          "quote": "we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau et al., 2020]."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "XNLI",
          "justification": "XNLI is used to evaluate the model's performance on cross-lingual natural language inference tasks.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins: average gains of +21.2% on XNLI."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "MLQA",
          "justification": "MLQA is used to evaluate the model's performance on cross-lingual question-answering tasks.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins...+33.8% on MLQA."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "XQuAD",
          "justification": "XQuAD is used to evaluate the model's performance on cross-lingual question-answering tasks.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins...+60.9% on XQuAD."
        },
        "role": "Used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "fairseq",
          "justification": "The experiments were implemented using the fairseq library, which is widely used for sequence modeling.",
          "quote": "Our experiments were implemented using fairseq [Ott et al., 2019]."
        },
        "role": "Used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 949,
    "prompt_tokens": 17390,
    "total_tokens": 18339
  }
}