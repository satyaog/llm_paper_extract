{
  "paper": "2308.12445.txt",
  "words": 10320,
  "extractions": {
    "title": {
      "value": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems",
      "justification": "The title is extracted directly from the provided paper information.",
      "quote": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"
    },
    "description": "The paper introduces Dr. DRL, a self-healing approach for deep reinforcement learning (DRL) systems. It leverages intentional forgetting to prioritize the adaptation of critical problem-solving skills in the face of environmental drifts. The approach uses continual learning (CL) combined with intentional forgetting to enhance the adaptability, reduce healing time, and improve performance of DRL agents in drifted environments.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research includes experimental evaluations of the proposed methodology on various environments, making it empirical.",
      "quote": "To demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments with different drifting intensities."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper discusses methods and experiments related to deep reinforcement learning, which is a subset of deep learning.",
      "quote": "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook."
    },
    "sub_research_fields": [
      {
        "value": "Reinforcement Learning",
        "justification": "The focus of the paper is specifically on deep reinforcement learning methods and their adaptability to environmental drifts.",
        "quote": "Deep reinforcement learning (DRL), the blend of deep learning (DL) and reinforcement learning (RL), has shown promising achievements in recent years."
      }
    ],
    "models": [
      {
        "name": {
          "value": "Deep Q-Learning (DQN)",
          "justification": "The model is explicitly mentioned in the paper as one of the DRL algorithms used for evaluation.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "caracteristics": [
          {
            "value": "DRL Algorithm",
            "justification": "This model is categorized as a deep reinforcement learning algorithm focused on maximizing cumulative rewards.",
            "quote": "Deep Q-Learning (DQN) [31, 48] is a popular value-based algorithm that leverages neural network to approximate the action-value function, Q."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "justification": "The model is explicitly mentioned in the paper as one of the DRL algorithms used for evaluation.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "caracteristics": [
          {
            "value": "DRL Algorithm",
            "justification": "This model is categorized as a deep reinforcement learning algorithm optimized for entropy and reward.",
            "quote": "Soft Actor-Critic (SAC) [32] is a hybrid DRL algorithm. It focuses on maximizing returns and policy entropy (i.e., degree of its 'randomness'), simultaneously."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Proximal Policy Optimization (PPO)",
          "justification": "The model is explicitly mentioned in the paper as one of the DRL algorithms used for evaluation.",
          "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "caracteristics": [
          {
            "value": "DRL Algorithm",
            "justification": "This model is categorized as a deep reinforcement learning algorithm designed for policy optimization and stability.",
            "quote": "Proximal Policy Optimization (PPO) [33] is a state-of-theart policy-based algorithm whose goal is to maximize policy optimization without compromising performance."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CartPole",
          "justification": "The dataset/environment is explicitly mentioned in the paper and used for evaluation purposes.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole [28], MountainCar [29], and Acrobot [30]"
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MountainCar",
          "justification": "The dataset/environment is explicitly mentioned in the paper and used for evaluation purposes.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole [28], MountainCar [29], and Acrobot [30]"
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Acrobot",
          "justification": "The dataset/environment is explicitly mentioned in the paper and used for evaluation purposes.",
          "quote": "three popular gym environments that we deliberately drift with varying parameters shifts, namely CartePole [28], MountainCar [29], and Acrobot [30]"
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The framework is explicitly mentioned as being used for implementing the proposed method.",
          "quote": "We implemented our approach as an open-source tool using Python 3.7 and it supports Tensorflow (version 2.4.4)."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1102,
    "prompt_tokens": 16852,
    "total_tokens": 17954
  }
}