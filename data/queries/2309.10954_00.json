{
  "paper": "2309.10954.txt",
  "words": 7266,
  "extractions": {
    "description": "This paper investigates the use of in-context learning (ICL) with large language models (LLMs) for text classification tasks with many labels. The approach leverages a pre-trained dense retrieval model to dynamically fetch relevant subsets of label examples and fit them into the limited context window of the LLM. The authors demonstrate state-of-the-art performance on multiple datasets without fine-tuning the models, and they explore the impact of various factors such as model size and number of in-context examples.",
    "title": {
      "value": "In-Context Learning for Text Classification with Many Labels",
      "justification": "The title directly matches the one provided in the research paper.",
      "quote": "In-Context Learning for Text Classification with Many Labels"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and analyzes performance using multiple datasets and models, which is indicative of empirical research.",
      "quote": "We evaluate LLMs in this setting with three intent classification datasets."
    },
    "research_field": {
      "value": "Natural Language Processing",
      "justification": "The paper focuses on text classification, which is a core task within the Natural Language Processing field.",
      "quote": "In-context learning (ICL) using large language models (LLMs) has recently exploded in popularity."
    },
    "sub_research_field": {
      "value": "Text Classification",
      "justification": "The study specifically targets text classification tasks, including intent classification and fine-grained sentiment analysis.",
      "quote": "In this work, we study whether ICL can handle challenging classification tasks with many possible labels."
    },
    "models": [
      {
        "name": {
          "value": "OPT",
          "justification": "OPT is one of the large language models evaluated in the study.",
          "quote": "Experiments are done using the LLaMA models (Touvron et al., 2023) and the OPT models (Zhang et al., 2022) as LLMs."
        },
        "role": "Used",
        "type": {
          "value": "Large Language Model",
          "justification": "OPT is categorized as a large language model.",
          "quote": "Testing with recent open-source LLMs (OPT, LLaMA)."
        },
        "mode": "Inference"
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA is another large language model evaluated in the study.",
          "quote": "Experiments are done using the LLaMA models (Touvron et al., 2023) and the OPT models (Zhang et al., 2022) as LLMs."
        },
        "role": "Used",
        "type": {
          "value": "Large Language Model",
          "justification": "LLaMA is categorized as a large language model.",
          "quote": "Testing with recent open-source LLMs (OPT, LLaMA)."
        },
        "mode": "Inference"
      },
      {
        "name": {
          "value": "DeBERTa-v2-XXLarge",
          "justification": "DeBERTa-v2-XXLarge is used as a baseline model in the comparative experiments.",
          "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the “Pfeiffer” bottleneck-style adapter)."
        },
        "role": "Used",
        "type": {
          "value": "Masked Language Model",
          "justification": "DeBERTa-v2-XXLarge is a masked language model used for comparison.",
          "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the “Pfeiffer” bottleneck-style adapter)."
        },
        "mode": "Inference"
      },
      {
        "name": {
          "value": "SBERT",
          "justification": "SBERT is used as a retrieval model to fetch relevant examples for in-context learning.",
          "quote": "The model we use is a contrastively trained model which has been pre-trained on a massive generic dataset of text pairs."
        },
        "role": "Used",
        "type": {
          "value": "Sentence Encoder",
          "justification": "SBERT is a sentence encoder-based retrieval model used to facilitate in-context learning.",
          "quote": "The retrieval model used is a Sentence-BERT model trained in a Siamese dual-network setup to be able to retrieve text based on cosine similarity of the embedding vectors it produces."
        },
        "mode": "Inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BANKING77",
          "justification": "BANKING77 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "HWU64",
          "justification": "HWU64 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "CLINC150",
          "justification": "CLINC150 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "GoEmotions",
          "justification": "GoEmotions is a fine-grained sentiment classification dataset used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "role": "Used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AdapterHub",
          "justification": "AdapterHub is used to implement the bottleneck-style adapter for the DeBERTa-v2-XXLarge model.",
          "quote": "DeBERTa-v2-XXLarge with the “Pfeiffer” bottleneck-style adapter (Pfeiffer et al., 2020b) implemented with AdapterHub."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "SentenceTransformers",
          "justification": "SentenceTransformers library is used to load the pre-trained retrieval model for the experiments.",
          "quote": "Specific retrieval model: For our sentence encoder/retriever, we use the SentenceTransformers library (Reimers and Gurevych, 2019a), and use the pre-trained “all-mpnet-base-v2” model."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "SetFit",
          "justification": "SetFit is another framework used for contrastive fine-tuning in the comparative experiments.",
          "quote": "The SetFit results are based on contrastively tuning the same pre-trained model trained by Microsoft through the Setfit library."
        },
        "role": "Used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1304,
    "prompt_tokens": 12823,
    "total_tokens": 14127
  }
}