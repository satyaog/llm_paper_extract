{
  "paper": "2302.09852.txt",
  "words": 15592,
  "extractions": {
    "description": "This paper proposes a data-driven, unsupervised method to improve Out-of-Distribution (OOD) detection for textual data by aggregating layer-wise anomaly scores from neural network encoders. It challenges the common assumption that the last layer's output is always the best for OOD detection and demonstrates that leveraging information from all layers yields better results. The authors also introduce a new benchmark, MILTOOD-C, which includes classification tasks with a larger number of classes and multiple languages to reflect more realistic settings.",
    "title": {
      "value": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"
    },
    "type": {
      "value": "Empirical",
      "justification": "The paper conducts extensive experiments to validate the proposed methods and introduces a new dataset for benchmarking.",
      "quote": "We propose a data-driven, unsupervised method to ... show that the proposed post-aggregation methods achieve robust and consistent results... Our experiments involve four models and over 186 pairs of IN and OUT datasets..."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper focuses on improving OOD detection methods using deep learning models.",
      "quote": "Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements... We propose a data-driven, unsupervised method... to combine layer-wise anomaly scores."
    },
    "sub_research_field": {
      "value": "Natural Language Processing",
      "justification": "The paper specifically addresses OOD detection in textual data, which falls under the sub-field of Natural Language Processing (NLP).",
      "quote": "Distinguishing OOD samples (OUT) from in-distribution (IN) samples is a challenge when working on complex data structures (e.g., text or image) due to their high dimensionality."
    },
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is mentioned as one of the models used for performing experiments in the study.",
          "quote": "Model selection. To ensure that our results are consistent ... we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions)"
        },
        "role": "used",
        "type": {
          "value": "Transformer",
          "justification": "BERT is a well-known transformer model.",
          "quote": "we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions)"
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "DISTILBERT",
          "justification": "DISTILBERT is also listed among the models used in the experiments.",
          "quote": "we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: ... DISTILBERT (Sanh et al. 2019)"
        },
        "role": "used",
        "type": {
          "value": "Transformer",
          "justification": "DISTILBERT is a variant of BERT, which is a transformer model.",
          "quote": "DISTILBERT (Sanh et al. 2019)."
        },
        "mode": "fine-tuned"
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is mentioned among the models used for fine-tuning.",
          "quote": "we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: ... RoBERTa (Liu et al. 2019) (base and large versions)"
        },
        "role": "used",
        "type": {
          "value": "Transformer",
          "justification": "RoBERTa is a transformer model, similar to BERT.",
          "quote": "RoBERTa (Liu et al. 2019) (base and large versions)"
        },
        "mode": "fine-tuned"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MILTOOD-C",
          "justification": "MILTOOD-C is a newly introduced benchmark dataset described extensively in the paper.",
          "quote": "We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "20 Newsgroups",
          "justification": "This dataset is mentioned as part of the English benchmark used in the experiments.",
          "quote": "English benchmark. We relied on the benchmark proposed by Zhou, Liu, and Chen (2021); Hendrycks et al. (2020). It features ... 20Newsgroup"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "IMDB",
          "justification": "IMDB dataset is used for sentiment analysis tasks within the experiments.",
          "quote": "English benchmark. We relied on the benchmark proposed by Zhou, Liu, and Chen (2021); Hendrycks et al. (2020). It features ... IMDB"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "SST2",
          "justification": "The SST2 dataset is included as part of the benchmark datasets.",
          "quote": "English benchmark. We relied on the benchmark proposed by Zhou, Liu, and Chen (2021); Hendrycks et al. (2020). It features ... SST2 (Socher et al. 2013)"
        },
        "role": "used"
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1323,
    "prompt_tokens": 36727,
    "total_tokens": 38050
  }
}