{
  "paper": "2401.11237.txt",
  "words": 11221,
  "extractions": {
    "description": "The paper investigates the stitching property in reinforcement learning (RL), which allows RL algorithms to generalize and solve tasks not seen during training. It compares RL methods based on dynamic programming with supervised learning (SL) based methods. The paper finds that SL-based methods often lack this stitching property but proposes a temporal data augmentation technique to improve their performance.",
    "title": {
      "value": "Closing the Gap Between TD Learning and Supervised Learning – A Generalization Point of View",
      "justification": "This title encapsulates the core comparison between Temporal Difference (TD) learning in RL and Supervised Learning approaches, with a focus on generalization.",
      "quote": "Closing the Gap Between TD Learning and Supervised Learning – A Generalisation Point of View"
    },
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents both theoretical analysis and empirical experiments to study the stitching property and the effectiveness of temporal data augmentation.",
      "quote": "Our empirical results support the theory: we demonstrate that prior RL methods based on SL (DT [2] and RvS [3]) fail to perform stitching, even when trained on abundant quantities of data."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The research focuses on improvements and comparisons in deep learning methodologies applied to reinforcement learning.",
      "quote": "Our work hints that current SL approaches may not efficiently use sequential data found in RL."
    },
    "sub_research_field": {
      "value": "Reinforcement Learning",
      "justification": "The research specifically targets techniques and properties within the realm of reinforcement learning, such as the stitching property.",
      "quote": "Our work shows that combinatorial generalisation is also required to solve tasks in the context of RL."
    },
    "models": [
      {
        "name": {
          "value": "DQN",
          "justification": "DQN is mentioned as an RL algorithm with the stitching property.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "role": "referenced",
        "type": {
          "value": "Reinforcement Learning",
          "justification": "DQN is an RL algorithm known for its use of Q-learning for dynamic programming.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "DDPG",
          "justification": "DDPG is cited as another RL method that has the stitching property.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "role": "referenced",
        "type": {
          "value": "Reinforcement Learning",
          "justification": "DDPG applies deep learning to deterministic policy gradients in RL settings.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "TD3",
          "justification": "TD3 is included as an example of an RL algorithm benefiting from the stitching property.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "role": "referenced",
        "type": {
          "value": "Reinforcement Learning",
          "justification": "TD3 improves upon DDPG by addressing function approximation errors.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "IQL",
          "justification": "IQL is mentioned as part of RL algorithms showing the stitching property.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "role": "referenced",
        "type": {
          "value": "Reinforcement Learning",
          "justification": "IQL is another RL algorithm that utilizes implicit Q-learning methods.",
          "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Offline Ant Maze",
          "justification": "The Offline Ant Maze dataset is used in the context of RL to evaluate the stitching property.",
          "quote": "In our experiments, we collect new offline datasets that precisely test for stitching (see Fig. 3 and Fig. 12 for visualisation)."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "scikit-learn",
          "justification": "Scikit-learn is mentioned as the library used for implementing the k-means clustering in the proposed data augmentation approach.",
          "quote": "We use the k-means algorithm from scikit-learn [62] with the default parameters to group states together."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1317,
    "prompt_tokens": 17700,
    "total_tokens": 19017
  }
}