{
  "paper": "1910.13466.txt",
  "words": 6906,
  "extractions": {
    "title": {
      "value": "Ordered Memory",
      "justification": "This is the exact title as provided in the text of the paper.",
      "quote": "Ordered Memory"
    },
    "description": "The paper proposes the Ordered Memory architecture, which includes a new attention-based mechanism inspired by Ordered Neurons. The architecture uses a cumulative probability to control memory operations and introduces a new Gated Recursive Cell. The model shows strong performance on logical inference and ListOps tasks and achieves competitive results in the Stanford Sentiment Treebank tasks.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves empirical experiments to demonstrate the performance of the Ordered Memory architecture on multiple tasks.",
      "quote": "We demonstrate that our model achieves strong performance on the logical inference task (Bowman et al., 2015) and the ListOps (Nangia and Bowman, 2018) task."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper focuses on enhancements and experiments in the field of deep learning through the Ordered Memory architecture.",
      "quote": "Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some time."
    },
    "sub_research_fields": [
      {
        "value": "Neural Network Architectures",
        "justification": "The paper deals with improvements and experiments based on neural network architectures, specifically recurrent neural networks with stack-like memory.",
        "quote": "In this paper, we propose the Ordered Memory architecture."
      }
    ],
    "models": [
      {
        "name": {
          "value": "Ordered Memory",
          "justification": "This is the new model architecture introduced and evaluated in the paper.",
          "quote": "In this paper, we propose the Ordered Memory architecture."
        },
        "caracteristics": [
          {
            "value": "Stack-augmented recurrent neural network",
            "justification": "The model is a stack-augmented recurrent neural network equipped with a new memory update mechanism and a Gated Recursive Cell.",
            "quote": "In this paper, we propose the Ordered Memory architecture."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ON-LSTM",
          "justification": "This model is mentioned for its conceptual similarities and as a basis of inspiration for the Ordered Memory model.",
          "quote": "Inspired by Ordered Neurons (Shen et al., 2018)..."
        },
        "caracteristics": [
          {
            "value": "LSTM",
            "justification": "The ON-LSTM is a variant of the Long Short Term Memory (LSTM) network with ordered neurons.",
            "quote": "Inspired by Ordered Neurons (Shen et al., 2018)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TreeLSTM",
          "justification": "TreeLSTM is used as a comparison model for evaluating the performance of the new Gated Recursive Cell.",
          "quote": "Instead of using the recursive cell proposed in TreeLSTM (Tai et al., 2015)..."
        },
        "caracteristics": [
          {
            "value": "LSTM",
            "justification": "TreeLSTM extends the standard LSTM to tree-structured topologies.",
            "quote": "Instead of using the recursive cell proposed in TreeLSTM (Tai et al., 2015)..."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "Transformer models are used as baselines for comparison in the logical inference experiments.",
          "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
        },
        "caracteristics": [
          {
            "value": "Attention-based neural network",
            "justification": "The Transformer is an attention-based model commonly used in NLP tasks.",
            "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Universal Transformer",
          "justification": "Universal Transformer models are used as baselines for comparison in the logical inference experiments.",
          "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
        },
        "caracteristics": [
          {
            "value": "Attention-based neural network",
            "justification": "The Universal Transformer is an extension of the Transformer architecture integrating recurrence.",
            "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Logical Inference",
          "justification": "The Logical Inference dataset is used to evaluate the generalization of the proposed Ordered Memory model.",
          "quote": "We demonstrate that our model achieves strong performance on the logical inference task (Bowman et al., 2015)..."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ListOps",
          "justification": "The ListOps dataset is used to evaluate the data efficiency and performance of the Ordered Memory model.",
          "quote": "We demonstrate that our model achieves strong performance on... the ListOps (Nangia and Bowman, 2018) task."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Stanford Sentiment Treebank",
          "justification": "The model was evaluated on the Stanford Sentiment Treebank to demonstrate its performance on sentiment analysis tasks.",
          "quote": "We also perform experiments on the Stanford Sentiment Treebank tasks (Socher et al., 2013), and find that we achieve comparative results to the current benchmarks."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Evalb",
          "justification": "The Evalb library is used to evaluate parsing performance using the F1 score.",
          "quote": "The F1 score is the parsing score with respect to the ground truth tree structure. The TreeCell is a recursive neural network based on the Gated Recursive Cell function proposed in section 3.2. For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018). The model takes <start> sentence1 <delim> sentence2 <extract> as input, then use the vector representation for <extract> position at last layer for classification. * The results for RRNet were taken from Jacob et al. (2018)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1522,
    "prompt_tokens": 11820,
    "total_tokens": 13342
  }
}