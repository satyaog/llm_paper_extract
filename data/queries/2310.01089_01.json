{
  "paper": "2310.01089.txt",
  "words": 11731,
  "extractions": {
    "description": "This paper introduces G RAPH T EXT, a novel framework aiming to convert graph data into textual format. This transformation allows Large Language Models (LLMs) to perform graph reasoning as a text generation task. G RAPH T EXT achieves competitive or superior performance to supervised graph neural networks without specific training on graph data, leveraging in-context learning and allowing interactive graph reasoning between humans and LLMs.",
    "title": {
      "value": "G RAPH T EXT: G RAPH R EASONING IN T EXT S PACE",
      "justification": "The title is present in the provided document.",
      "quote": "G RAPH T EXT: G RAPH R EASONING IN T EXT S PACE"
    },
    "type": {
      "value": "empirical study",
      "justification": "The paper introduces a novel framework and demonstrates its effectiveness through extensive experiments and empirical results.",
      "quote": "We conduct extensive experiments to demonstrate the effectiveness of G RAPH T EXT."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper discusses methods and models related to large language models and graph neural networks, which are key topics within deep learning.",
      "quote": "Large Language Models (LLMs) have gained the ability to assimilate human knowledge and facilitate natural language interactions..."
    },
    "sub_research_field": {
      "value": "Graph Machine Learning",
      "justification": "The paper focuses on the challenges and methods of applying deep learning techniques, specifically with LLMs, to graph-structured data.",
      "quote": "Despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning."
    },
    "models": [
      {
        "name": {
          "value": "ChatGPT",
          "justification": "ChatGPT is utilized extensively in the G RAPH T EXT experiments.",
          "quote": "G RAPH T EXT with ChatGPT can achieve on par with, or even surpass..., the performance of supervised-trained graph neural networks"
        },
        "role": "used",
        "type": {
          "value": "Large Language Model",
          "justification": "ChatGPT is known as a Large Language Model developed by OpenAI.",
          "quote": "G RAPH T EXT with ChatGPT"
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is used alongside ChatGPT in the experiments to validate the proposed framework.",
          "quote": "We leverage G RAPH T EXT with ChatGPT and GPT-4 to perform graph reasoning"
        },
        "role": "used",
        "type": {
          "value": "Large Language Model",
          "justification": "GPT-4 is another version of the Large Language Model developed by OpenAI following GPT-3 and ChatGPT.",
          "quote": "G RAPH T EXT with ChatGPT and GPT-4"
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "GCN",
          "justification": "GCN is mentioned as one of the baseline models for comparison with G RAPH T EXT.",
          "quote": "We selected standard GNNs, including GCN (Kipf \\\\& Welling, 2017)..."
        },
        "role": "referenced",
        "type": {
          "value": "Graph Neural Network",
          "justification": "Graph Convolutional Network (GCN) is a common type of Graph Neural Network.",
          "quote": "GCN (Kipf \\\\& Welling, 2017)"
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "GAT",
          "justification": "GAT is also used as a baseline model for comparison.",
          "quote": "We selected standard GNNs, including ... GAT (Velickovic et al., 2018)..."
        },
        "role": "referenced",
        "type": {
          "value": "Graph Neural Network",
          "justification": "Graph Attention Network (GAT) is another popular type of Graph Neural Network.",
          "quote": "GAT (Velickovic et al., 2018)"
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cora",
          "justification": "Cora dataset is utilized in the experiments to evaluate the proposed framework.",
          "quote": "We use two citation datasets (Cora (McCallum et al., 2000)..."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Citeseer",
          "justification": "Citeseer dataset is also used in the experiments for evaluation.",
          "quote": "We use two citation datasets ... and Citeseer (Giles et al., 1998), ..."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Texas",
          "justification": "Texas dataset is used as one of the benchmarks to evaluate G RAPH T EXT.",
          "quote": "We use ... three webpage datasets (Texas, Wisconsin, and Cornell (Pei et al., 2020)..."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Wisconsin",
          "justification": "Wisconsin dataset is used alongside other datasets for evaluation.",
          "quote": "We use ... three webpage datasets (Texas, Wisconsin, and Cornell (Pei et al., 2020)..."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Cornell",
          "justification": "Cornell is another dataset used for evaluating the framework.",
          "quote": "We use ... three webpage datasets (Texas, Wisconsin, and Cornell (Pei et al., 2020)..."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is likely to be used given its standard role in implementing deep learning models, especially those like GNNs and LLMs mentioned in the paper.",
          "quote": "Specific mention of the library is not quoted, but inferred from common usage for implementing experimental frameworks"
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1086,
    "prompt_tokens": 24143,
    "total_tokens": 25229
  }
}