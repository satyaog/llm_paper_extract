{
  "paper": "2310.08513.txt",
  "words": 14576,
  "extractions": {
    "description": "This paper investigates the impact of initial weight structures, particularly their effective rank, on the learning dynamics of neural networks. It demonstrates that high-rank initializations lead to lazier learning, whereas low-rank initializations result in richer learning. These findings are grounded in theoretical derivations and empirical validations using recurrent neural networks (RNNs) on various tasks.",
    "title": {
      "value": "How Connectivity Structure Shapes Rich and Lazy Learning in Neural Circuits",
      "justification": "The title directly reflects the paper's investigation into how different initial weight structures affect the learning regimes in neural networks, with 'rich' and 'lazy' learning as the core focus.",
      "quote": "H OW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS"
    },
    "type": {
      "value": "theoretical study",
      "justification": "The paper includes theoretical derivations in a two-layer feedforward linear network and extends these results empirically, focusing on the theory behind network initialization and its effects.",
      "quote": "Through theoretical derivation in two-layer feedforward linear network, we demonstrate that higher-rank initialization results in effectively lazier learning on average across tasks"
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The study revolves around neural networks, a fundamental aspect of Deep Learning, and explores their learning dynamics based on initial weight structures.",
      "quote": "In deep learning, structure, encompassing architecture and initial connectivity, crucially dictates learning speed and effectiveness"
    },
    "sub_research_field": {
      "value": "Neural Network Learning Dynamics",
      "justification": "The paper specifically addresses how the learning dynamics of neural networks are influenced by their initial weight structures, which falls under the study of neural network learning dynamics.",
      "quote": "here we investigate how the structure of the initial weights \\\\u2014 in particular their effective rank \\\\u2014 influences the network learning regime"
    },
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Networks (RNNs)",
          "justification": "RNNs are used to validate the theoretical findings through numerical experiments on neuroscience tasks.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks"
        },
        "role": "used",
        "type": {
          "value": "Recurrent Neural Networks",
          "justification": "RNNs are a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence, allowing them to exhibit temporal dynamic behavior.",
          "quote": "We examine recurrent neural networks (RNNs) because they are commonly adopted for modeling neural circuits"
        },
        "mode": "trained"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Neurogym",
          "justification": "The paper uses Neurogym tasks like two-alternative forced choice, delayed-match-to-sample, and context-dependent decision-making to validate their theoretical findings in RNNs.",
          "quote": "For our investigations, we applied this initialization scheme across a variety of cognitive tasks \\\\u2014 including two-alternative forced choice (2AF), delayed-match-to-sample (DMS), context-dependent decision-making (CXT) tasks \\\\u2014 implemented with Neurogym"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "Sequential MNIST (sMNIST)",
          "justification": "sMNIST is employed to demonstrate the findings on a well-known machine learning benchmark task.",
          "quote": "and the well-known machine learning benchmark sequential MNIST (sMNIST). Figure 1 indicates that low-rank initial weights result in effectively richer learning and greater network changes."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper utilizes PyTorch for implementing and training the RNN models used in their experiments.",
          "quote": "We used PyTorch Version 1.10.2"
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 757,
    "prompt_tokens": 24510,
    "total_tokens": 25267
  }
}