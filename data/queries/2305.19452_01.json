{
  "paper": "2305.19452.txt",
  "words": 9099,
  "extractions": {
    "description": "This paper introduces a new value-based RL agent, called BBF, which achieves super-human performance in the Atari 100K benchmark. BBF scales the neural networks used for value estimation in a sample-efficient manner and incorporates various design choices to enable this scaling. The authors also discuss moving the goalposts for sample-efficient RL research on the ALE benchmark.",
    "title": {
      "value": "Bigger, Better, Faster: Human-level Atari with human-level efficiency",
      "justification": "The title is directly taken from the research paper.",
      "quote": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive analyses and experiments to evaluate the BBF model on the Atari 100K benchmark and compares it with other models.",
      "quote": "We conduct extensive analyses of these design choices and provide insights for future work."
    },
    "research_field": {
      "value": "Deep Reinforcement Learning",
      "justification": "The paper focuses on value-based RL agents and their performance in the Atari 100K benchmark, which falls under deep reinforcement learning.",
      "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
    },
    "sub_research_field": {
      "value": "Model-Free Reinforcement Learning",
      "justification": "The paper emphasizes the development and evaluation of a model-free RL agent, BBF, in a sample-efficient manner.",
      "quote": "While human-level efficiency has been obtained by the model-based EfficientZero agent (Ye et al., 2021), it has remained elusive for model-free RL agents."
    },
    "models": [
      {
        "name": {
          "value": "BBF",
          "justification": "The BBF model is the main contribution of the paper, and it's thoroughly analyzed and compared with other RL agents.",
          "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
        },
        "role": "Contributed",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "BBF is explicitly described as a model-free RL agent in the paper.",
          "quote": "To this end, we introduce BBF, a model-free RL agent that achieves super-human performance."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "AlphaStar",
          "justification": "Referenced as an example of successful RL methods.",
          "quote": "... playing complex games at a human or super-human level, such as ... AlphaStar (Vinyals et al., 2019) ..."
        },
        "role": "Referenced",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "AlphaStar is recognized for its model-free RL technique in playing StarCraft II.",
          "quote": "..., such as ... AlphaStar (Vinyals et al., 2019) ..."
        },
        "mode": "N/A"
      },
      {
        "name": {
          "value": "DER",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DER (Van Hasselt et al., 2019) ..."
        },
        "role": "Used",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "DER is identified as a model-free RL agent in the experimental comparison.",
          "quote": "Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DER ..."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "DrQ",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DrQ (Kostrikov et al., 2020) ..."
        },
        "role": "Used",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "DrQ is identified as a model-free RL agent in the experimental comparison.",
          "quote": "Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DrQ ..."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "SPR",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... SPR (Schwarzer et al., 2021) ..."
        },
        "role": "Used",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "SPR is identified as a model-free RL agent in the experimental comparison.",
          "quote": "Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... SPR ..."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "OpenAI Five",
          "justification": "The model is referenced as an example of a successful RL method.",
          "quote": "... playing complex games at a human or super-human level, such as OpenAI Five (Berner et al., 2019), ..."
        },
        "role": "Referenced",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "OpenAI Five is well known for its model-free RL approach to playing Dota 2.",
          "quote": "... playing complex games at a human or super-human level, such as OpenAI Five (Berner et al., 2019), ..."
        },
        "mode": "N/A"
      },
      {
        "name": {
          "value": "AlphaGo",
          "justification": "Referenced as an example of successful RL methods.",
          "quote": "... playing complex games at a human or super-human level, such as AlphaGo (Silver et al., 2016), ..."
        },
        "role": "Referenced",
        "type": {
          "value": "Model-Free Reinforcement Learning Agent",
          "justification": "AlphaGo uses a combination of deep neural networks and Monte Carlo tree search in a model-free RL approach.",
          "quote": "... playing complex games at a human or super-human level, such as AlphaGo (Silver et al., 2016), ..."
        },
        "mode": "N/A"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Kinetics-700",
          "justification": "The dataset is referenced in the paper for performance evaluation in the broader context of RL.",
          "quote": "... the learned policies may be unable to handle distribution shifts when interacting with the real environment (Levine et al., 2020)..."
        },
        "role": "Referenced"
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "Discussed in the context of general performance evaluation standards.",
          "quote": "... improving generalization in RL using datasets like CIFAR-10 ..."
        },
        "role": "Referenced"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "Used for implementing some of the models and experiments in the paper.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including JAX (Bradbury et al., 2018) ..."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "NumPy",
          "justification": "Used for numerical operations in the implementation.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including NumPy (Harris et al., 2020) ..."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Matplotlib",
          "justification": "Used for plotting and visualizations in the paper.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including ... Matplotlib (Hunter, 2007) ..."
        },
        "role": "Used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1827,
    "prompt_tokens": 17369,
    "total_tokens": 19196
  }
}