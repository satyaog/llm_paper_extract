{
  "paper": "2404.09939.txt",
  "words": 12371,
  "extractions": {
    "description": "This paper offers a comprehensive survey of deep learning techniques applied to the field of theorem proving, reviewing existing approaches, available datasets, evaluation metrics, and suggesting future research directions.",
    "title": {
      "value": "A Survey on Deep Learning for Theorem Proving",
      "justification": "The title succinctly describes the content and focus of the paper.",
      "quote": "A Survey on Deep Learning for Theorem Proving"
    },
    "type": {
      "value": "empirical",
      "justification": "The paper provides a thorough review and analysis of existing deep learning methods, datasets, and performance metrics within the specific field of theorem proving.",
      "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering i) a thorough review of existing approaches... ii) a meticulous summary of available datasets... iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The survey focuses on the application of deep learning techniques within the domain of theorem proving.",
      "quote": "The recent development of deep learning, especially with the evolution of large language models (LLMs), has ignited a wave of research interest in this area again."
    },
    "sub_research_field": {
      "value": "Theorem Proving",
      "justification": "The sub-research field explicitly narrows down to the intersection of theorem proving and deep learning methods.",
      "quote": "Exploring learning-based approaches for theorem proving has been a long-standing research focus..."
    },
    "models": [
      {
        "name": {
          "value": "PaLM",
          "justification": "PaLM is mentioned as being studied for autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023)..."
        },
        "role": "referenced",
        "type": {
          "value": "Large Language Model",
          "justification": "PaLM is referred to as a large language model being studied for its application in autoformalization.",
          "quote": "The recent development of LLMs and their in-context learning capabilities... using PaLM (Chowdhery et al., 2023)..."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is discussed for its use in autoformalization.",
          "quote": "Wu et al. (2022); Azerbayev et al. (2023); Jiang et al. (2023a) explore advanced LLMs like Codex and GPT-4 (Achiam et al., 2023) for informalization..."
        },
        "role": "referenced",
        "type": {
          "value": "Large Language Model",
          "justification": "Codex is specified as a large language model used in the context of translating formal statements to natural language.",
          "quote": "Recently, Codex (Chen et al., 2021) has... been studied for autoformalization."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is highlighted for its contributions to autoformalization and proof generation.",
          "quote": "Using GPT-4, MMA (Jiang et al., 2023a) informalizes all theorem statements in Archive of Formal Proofs and mathlib..."
        },
        "role": "referenced",
        "type": {
          "value": "Large Language Model",
          "justification": "GPT-4 is recognized as a large language model aiding in both autoformalization and theorem proving tasks.",
          "quote": "state-of-the-art LLMs like GPT-4 in structured frameworks than tactic-based models..."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "AlphaGeometry",
          "justification": "AlphaGeometry is noted for its application in the domain of geometry theorem proving.",
          "quote": "Notably, AlphaGeometry (Trinh et al., 2024) trains a decoder-only Transformer to predict the auxiliary constructions in the proofs of International Mathematical Olympiad (IMO) geometry problems."
        },
        "role": "referenced",
        "type": {
          "value": "Decoder-Only Transformer",
          "justification": "AlphaGeometry is a specific instantiation of a model aimed at solving geometry problems through a deep learning approach.",
          "quote": "Trains a decoder-only Transformer to predict the auxiliary constructions in the proofs of International Mathematical Olympiad (IMO) geometry problems."
        },
        "mode": "training"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "NL-PS",
          "justification": "NL-PS is mentioned as a dataset designed for premise selection from natural language.",
          "quote": "NL-PS (Ferreira & Freitas, 2020a) first builds a natural language premise selection dataset sourced from ProofWiki."
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "NaturalProofs",
          "justification": "NaturalProofs is referred to as a dataset that includes data from various sources for better diversity.",
          "quote": "Similarly, NaturalProofs (Welleck et al., 2021) further incorporates data from Stacks and textbooks, resulting in a dataset with roughly 25k examples."
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "GamePad",
          "justification": "GamePad is highlighted as a Coq dataset for formal proofs.",
          "quote": "Notable datasets for Coq include Gamepad (Huang et al., 2019)..."
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "CoqGym",
          "justification": "CoqGym is recognized specifically as a large-scale dataset for Coq proofs.",
          "quote": "with CoqGym constructing a dataset from 123 projects encompassing 71k proofs."
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "IsarStep",
          "justification": "IsarStep is mentioned for its utility in formal theorem proving within Isabelle.",
          "quote": "For Isabelle, datasets like IsarStep (Li et al., 2021a)... are built on the Archive of Formal Proofs and Isabelle Standard Library..."
        },
        "role": "referenced"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Isabelle",
          "justification": "Isabelle is highlighted as a proof assistant utilized in interactive theorem proving.",
          "quote": "Isabelle (Paulson, 1994), HOL Light (Harrison, 1996), Coq (Barras et al., 1999), and Lean (Moura & Ullrich, 2021)"
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "HOL Light",
          "justification": "HOL Light is recognized for its role as a proof assistant in formal theorem proving.",
          "quote": "HOL Light (Harrison, 1996)"
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "Coq",
          "justification": "Coq is specifically identified as a proof assistant used extensively in the research.",
          "quote": "Coq (Barras et al., 1999)"
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "Lean",
          "justification": "Lean is highlighted as one of the major proof assistants used in formal theorem proving.",
          "quote": "Lean (Moura & Ullrich, 2021)"
        },
        "role": "referenced"
      },
      {
        "name": {
          "value": "mathlib",
          "justification": "mathlib is mentioned as a significant library used within the Lean proof assistant.",
          "quote": "Lean's mathlib library (mathlib Community, 2020)"
        },
        "role": "referenced"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1831,
    "prompt_tokens": 22925,
    "total_tokens": 24756
  }
}