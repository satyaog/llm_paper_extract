{
  "paper": "2309.16650.txt",
  "words": 10191,
  "extractions": {
    "description": "The paper proposes ConceptGraphs, a novel method to construct open-vocabulary and object-centric 3D scene graphs for robot perception and planning. It leverages 2D foundation models and fuses their outputs into 3D by multiview association to create a structured and semantically rich representation. The method allows robots to understand complex scenes, perform task planning, and handle abstract language queries without requiring large 3D datasets or model finetuning.",
    "title": {
      "value": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
      "justification": "The title precisely describes the primary contribution of the paper, which is the ConceptGraphs method.",
      "quote": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and real-world trials with robots to validate the proposed method, which classifies it as an empirical study.",
      "quote": "We implement and demonstrate ConceptGraphs on a number of real-world robotics tasks across wheeled and legged mobile robot platforms."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The research involves using deep learning models for building 3D scene representations from 2D image data.",
      "quote": "Recent approaches have attempted to leverage features from large vision-language models to encode semantics in 3D representations."
    },
    "sub_research_field": {
      "value": "3D Scene Understanding",
      "justification": "The paper focuses on 3D scene graphs which are an advanced representation for understanding the spatial relationships and semantic information in 3D scenes.",
      "quote": "ConceptGraphs, a 3D scene representation method for robot perception and planning that satisfies all the above requirements."
    },
    "models": [
      {
        "name": {
          "value": "Segment Anything (SAM)",
          "justification": "The model is used for segmentation in the proposed ConceptGraphs method.",
          "quote": "We use SegmentAnything (SAM) [33] as the segmentation model Seg(·)."
        },
        "role": "used",
        "type": {
          "value": "Segmentation Model",
          "justification": "SAM is employed to perform segmentation tasks within the ConceptGraphs pipeline.",
          "quote": "We use SegmentAnything (SAM) [33] as the segmentation model Seg(·)."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "The model extracts visual features for the segmented regions.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
        },
        "role": "used",
        "type": {
          "value": "Vision-Language Model",
          "justification": "CLIP is used for extracting visual features and embeddings within the ConceptGraphs framework.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "DINO",
          "justification": "The model is used alongside CLIP for feature extraction from segmented regions.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
        },
        "role": "used",
        "type": {
          "value": "Vision Transformer",
          "justification": "DINO, a vision transformer, is used for feature extraction tasks in the paper.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "LLaVA",
          "justification": "LLaVA is used for generating object captions from visual data.",
          "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
        },
        "role": "used",
        "type": {
          "value": "Vision-Language Model",
          "justification": "LLaVA is employed to interpret visual data and generate captions for identified objects.",
          "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is used as the language model for inferring relationships and processing language queries.",
          "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
        },
        "role": "used",
        "type": {
          "value": "Large Language Model",
          "justification": "GPT-4 is utilized for processing language-based tasks and queries within the ConceptGraphs framework.",
          "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "Grounding DINO",
          "justification": "The model is used in a variant of the ConceptGraphs system for open-vocabulary detection.",
          "quote": "An open-vocabulary 2D detector (Grounding DINO [34]) to obtain object bounding boxes."
        },
        "role": "referenced",
        "type": {
          "value": "Detection Model",
          "justification": "Grounding DINO is referenced but not the main focus model used for detection in ConceptGraphs.",
          "quote": "An open-vocabulary 2D detector (Grounding DINO [34]) to obtain object bounding boxes."
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Replica",
          "justification": "The dataset is used to evaluate the accuracy of 3D scene graphs generated by ConceptGraphs.",
          "quote": "For each scene in the Replica dataset [56], we report scene graph accuracy metrics for both CG and the detector-variant CG-D."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Open3d SLAM",
          "justification": "Used for building initial pointcloud maps for the Jackal robot navigation.",
          "quote": "The initial Jackal pointcloud does not include task-relevant objects and is downprojected to a 2D costmap for navigation using the base Jackal ROS stack."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "RTAB-Map",
          "justification": "Utilized for obtaining camera poses and the scene point cloud in experiments.",
          "quote": "We then stage two separate scenes with different objects: one for object search and another for traversability estimation. In both cases, we map the scene with an Azure Kinect Camera and rely on RTAB-Map [69] to obtain camera poses and the scene point cloud."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1338,
    "prompt_tokens": 17061,
    "total_tokens": 18399
  }
}