{
  "paper": "2401.07927.txt",
  "words": 26872,
  "extractions": {
    "description": "The paper investigates whether self-explanations provided by large language models (LLMs) are faithful to the model's behavior. It proposes using self-consistency checks to measure faithfulness and tests this on various models and tasks.",
    "title": {
      "value": "Are self-explanations from Large Language Models faithful?",
      "justification": "The extracted title matches exactly as given in the user prompt.",
      "quote": "Are self-explanations from Large Language Models faithful?"
    },
    "type": {
      "value": "empirical",
      "justification": "The research involves experimental evaluation of language models and their explanations, making it empirical.",
      "quote": "In this paper, we evaluate the faithfulness of the following types of self-explanations."
    },
    "research_field": {
      "value": "Natural Language Processing",
      "justification": "The paper primarily deals with the interpretability and faithfulness of explanations generated by NLP models.",
      "quote": "Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
    },
    "sub_research_field": {
      "value": "Interpretability",
      "justification": "The focus of the paper is on evaluating the faithfulness of self-explanations generated by LLMs, which falls under interpretability in NLP.",
      "quote": "Our investigation reveals that self-explanationsâ€™ faithfulness is highly model and dataset-dependent."
    },
    "models": [
      {
        "name": {
          "value": "Llama2",
          "justification": "The paper explicitly mentions using Llama2 for evaluating self-explanations.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2."
        },
        "role": "used",
        "type": {
          "value": "Large Language Model (LLM)",
          "justification": "Llama2 is referred to as a large language model in the paper.",
          "quote": "Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023)"
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "Falcon",
          "justification": "Falcon is one of the models used for evaluating faithfulness in self-explanations.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B."
        },
        "role": "used",
        "type": {
          "value": "Large Language Model (LLM)",
          "justification": "Falcon is referred to as a large language model in the paper.",
          "quote": "Instruction-tuned large language models (LLMs), such as...Falcon (Penedo et al., 2023)"
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "Mistral",
          "justification": "Mistral is another model evaluated for its faithfulness in providing self-explanations.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B."
        },
        "role": "used",
        "type": {
          "value": "Large Language Model (LLM)",
          "justification": "Mistral is referred to as a large language model in the paper.",
          "quote": "Instruction-tuned large language models (LLMs), such as...Mistral (Jiang et al., 2023)"
        },
        "mode": "inference"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "IMDB",
          "justification": "The paper uses the IMDB dataset for evaluating faithfulness in sentiment classification tasks.",
          "quote": "For example, regarding Llama2 (70B), counterfactuals only work with IMDB."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "bAbI",
          "justification": "The paper uses the bAbI dataset for multi-choice classification tasks.",
          "quote": "multi-choice classification (bAbI...Weston et al. 2016)"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "MCTest",
          "justification": "The paper uses the MCTest dataset for evaluating faithfulness in multi-choice classification tasks.",
          "quote": "multi-choice classification (bAbI and MCTest Weston et al. 2016; Richardson et al. 2013)"
        },
        "role": "used"
      },
      {
        "name": {
          "value": "RTE",
          "justification": "The paper uses the RTE dataset for evaluating faithfulness in two-paragraph classification tasks.",
          "quote": "two-paragraph classification (RTE Dagan et al. 2006)."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face",
          "justification": "The paper uses the Hugging Face library for model implementation and experimentation.",
          "quote": "Many of the models are or have been publically available at https://huggingface.co/chat"
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 985,
    "prompt_tokens": 41351,
    "total_tokens": 42336
  }
}