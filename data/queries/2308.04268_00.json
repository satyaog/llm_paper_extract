{
  "paper": "2308.04268.txt",
  "words": 16813,
  "extractions": {
    "description": "The paper provides a comprehensive review of Teacher-Student architectures for various knowledge distillation (KD) objectives, including knowledge compression, expansion, adaptation, and enhancement. It discusses different knowledge representations and optimization objectives, summarizes representative learning algorithms and effective distillation schemes, and explores recent applications of Teacher-Student architectures in multiple domains such as classification, recognition, generation, ranking, and regression. The paper aims to provide insights and guidelines for designing, learning, and applying Teacher-Student architectures across various distillation objectives.",
    "title": {
      "value": "Teacher-Student Architecture for Knowledge Distillation: A Survey",
      "justification": "The provided title accurately represents the focus of the paper, which surveys the Teacher-Student architecture for knowledge distillation.",
      "quote": "Teacher-Student Architecture for Knowledge Distillation: A Survey"
    },
    "type": {
      "value": "Empirical study",
      "justification": "The paper conducts a comprehensive review and summarizes practical applications, representative learning algorithms, and effective distillation schemes, making it an empirical study.",
      "quote": "Different from the existing KD surveys that primarily focus on knowledge compression, this survey first explores Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various knowledge representations and their corresponding optimization objectives."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The focus of the paper is on Teacher-Student architectures for knowledge distillation, which falls under the field of Deep Learning.",
      "quote": "Although Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters."
    },
    "sub_research_field": {
      "value": "Knowledge Distillation",
      "justification": "The entire paper is dedicated to surveying different aspects of knowledge distillation using Teacher-Student architectures.",
      "quote": "To tackle this issue, Teacher-Student architectures were proposed, where simple student networks with a few parameters can achieve comparable performance to deep teacher networks with many parameters. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge distillation (KD) objectives."
    },
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is used as an example of a model in the text classification tasks within the NLP domain, and it's referenced in the context of Knowledge Distillation.",
          "quote": "Tang et al. [153] suggest task-specific KD from a BERT teacher model to a bidirectional long short-term memory network for sentence classification and matching."
        },
        "role": "referenced",
        "type": {
          "value": "NLP Model",
          "justification": "BERT is a well-known model used for natural language processing (NLP) tasks.",
          "quote": "Tang et al. [153] suggest task-specific KD from a BERT teacher model to a bidirectional long short-term memory network for sentence classification and matching."
        },
        "mode": "inference"
      },
      {
        "name": {
          "value": "Bert-LSTM",
          "justification": "Bidirectional long short-term memory network (Bi-LSTM) is used as a student model to which knowledge is distilled from a BERT model.",
          "quote": "Tang et al. [15] compress BERT [16] to a much light-weight Bi-LSTM [17] for the task of natural language processing."
        },
        "role": "used",
        "type": {
          "value": "NLP Model",
          "justification": "Bi-LSTM is another type of NLP model used in the experiments mentioned in the paper.",
          "quote": "Tang et al. [15] compress BERT [16] to a much light-weight Bi-LSTM [17] for the task of natural language processing."
        },
        "mode": "trained"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as an example in the knowledge expansion objective where the teacher model is initially trained on ImageNet.",
          "quote": "Xie et al. [5] initially train the teacher model on ImageNet [25], then incorporate a privately collected unlabelled dataset, and use the teacher modelâ€™s prediction as pseudo ground truth to train a larger student model."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Torch",
          "justification": "Torch is likely used or referenced in the implementations discussed in the paper, as it is a common deep learning library for implementing models like BERT and Bi-LSTM.",
          "quote": "Deep neural networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs are hard to be deployed in real-world systems due to their voluminous parameters."
        },
        "role": "referenced"
      }
    ]
  },
  "usage": {
    "completion_tokens": 905,
    "prompt_tokens": 27162,
    "total_tokens": 28067
  }
}