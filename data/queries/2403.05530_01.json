{
  "paper": "2403.05530.txt",
  "words": 30556,
  "extractions": {
    "description": "This paper presents Gemini 1.5 Pro, a multimodal mixture-of-experts model capable of recalling and reasoning over information from millions of tokens of context, achieving high performance in long-context tasks across text, video, and audio.",
    "title": {
      "value": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "justification": "The title is clearly stated on the first page of the paper.",
      "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
    },
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides empirical results on the performance of Gemini 1.5 Pro and compares it with other models.",
      "quote": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, ... achieving high performance in long-context tasks across text, video, and audio."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper focuses on developing and evaluating a deep learning model, specifically a multimodal mixture-of-experts model.",
      "quote": "We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro. This is our first release from Gemini 1.5, a new family of highly-capable multimodal models which incorporates a novel mixture-of-experts architecture..."
    },
    "sub_research_field": {
      "value": "Multimodal Learning",
      "justification": "The research involves a model handling multiple data modalities including text, video, and audio.",
      "quote": "Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio."
    },
    "models": [
      {
        "name": {
          "value": "Gemini 1.5 Pro",
          "justification": "This is the main model introduced and evaluated in the paper.",
          "quote": "We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro."
        },
        "role": "Contributed",
        "type": {
          "value": "Multimodal mixture-of-experts model",
          "justification": "The paper describes Gemini 1.5 Pro as a multimodal mixture-of-experts model.",
          "quote": "Gemini 1.5 Pro... a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information..."
        },
        "mode": "Trained"
      },
      {
        "name": {
          "value": "Gemini 1.0 Ultra",
          "justification": "This model is used for comparison with the proposed model, Gemini 1.5 Pro.",
          "quote": "Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks..."
        },
        "role": "Referenced",
        "type": {
          "value": "Multimodal model",
          "justification": "Gemini 1.0 Ultra is referenced as a state-of-the-art multimodal model for comparison.",
          "quote": "Despite Gemini 1.5 Pro using significantly less training compute and being more efficient to serve, we find Gemini 1.5 Pro to perform better on more than half of the benchmarks (19/33), in particular on text (12/15) and many of the vision benchmarks (6/13)."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "Claude 2.1",
          "justification": "This model is used for comparison in the experiments evaluating long-context capabilities.",
          "quote": "Gemini 1.5 Pro achieves a 100% recall at 200k tokens, surpassing Claude 2.1’s 98%. This 100% recall is maintained up to 530k tokens, and recall is 99.7% at 1M tokens."
        },
        "role": "Referenced",
        "type": {
          "value": "Text-only model",
          "justification": "Claude 2.1 is referenced as a state-of-the-art long-context text-only model.",
          "quote": "Gemini 1.5 Pro significantly extends this context length frontier to multiple millions of tokens with almost no degradation in performance, making it possible to process significantly larger inputs. Compared to Claude 2.1 with a 200k token context window..."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "GPT-4 Turbo",
          "justification": "This model is used for comparison in the experiments evaluating long-context capabilities.",
          "quote": "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V (Balažević et al., 2024))."
        },
        "role": "Referenced",
        "type": {
          "value": "Text-only model",
          "justification": "GPT-4 Turbo is referenced as a state-of-the-art long-context text-only model.",
          "quote": "Gemini 1.5 Pro compared to Claude 2.1 and GPT-4 Turbo... Gemini 1.5 Pro achieves state-of-the-art performance in text needle-in-a-haystack tasks."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "GPT-4V",
          "justification": "This model is used for comparison in the video needle-in-a-haystack task.",
          "quote": "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V (Balažević et al., 2024))."
        },
        "role": "Referenced",
        "type": {
          "value": "Multimodal model",
          "justification": "GPT-4V is referenced as a multimodal model handling video inputs, used for comparison.",
          "quote": "GPT-4V API supports video lengths only up to around the first 3 minutes, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages for the full hour, as shown by the all-green plot."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "USM",
          "justification": "This model is used for comparison in automatic speech recognition tasks.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023)... achieving a WER of 8.8%."
        },
        "role": "Referenced",
        "type": {
          "value": "Multimodal model",
          "justification": "USM is referenced for comparison in audio understanding tasks.",
          "quote": "Gemini 1.5 Pro is much more robust on these longer-context tasks. Specifically, thanks to its long-context capabilities and without the added complexity of extra input segmentation and pre-processing, Gemini 1.5 Pro can transcribe 15-minute videos more accurately than other models, achieving a WER of 5.6%."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "Whisper",
          "justification": "This model is used for comparison in automatic speech recognition tasks.",
          "quote": "Note that ASR tasks report a word error rate (WER) metric, where a lower number is better. The Table 5 below shows that... Whisper is not robust to long segments and hence requires audio to be segmented every 30 seconds to achieve a WER of 7.3%."
        },
        "role": "Referenced",
        "type": {
          "value": "Text-only model",
          "justification": "Whisper is referenced for comparison in automatic speech recognition tasks.",
          "quote": "Unlike Gemini 1.5 Pro, existing models cannot natively handle more than a few seconds of audio in the context. Hence, in order to fairly compare against them we need to employ a strategy where we first transcribe audio into text using windows of tens of seconds, and then rely on text models to extend beyond that limited window."
        },
        "mode": "Used"
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "This model is used for comparison in terms of training data memorization.",
          "quote": "We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack."
        },
        "role": "Referenced",
        "type": {
          "value": "Text-only model",
          "justification": "LLaMA is referenced for comparison in text memorization tasks.",
          "quote": "We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack."
        },
        "mode": "Used"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MATH",
          "justification": "This dataset is used to evaluate mathematical problem-solving capabilities.",
          "quote": "We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e., +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH)..."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "Les Misérables",
          "justification": "This dataset is used for long-document QA tasks in the paper.",
          "quote": "We test the model's ability to answer them correctly when the entire 1,462 page book (i.e., 710K tokens) is provided as input."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "FLEURS",
          "justification": "This dataset is used to evaluate automatic speech recognition in multiple languages.",
          "quote": "On FLEURS we evaluate a subset of 55 languages for which we have coverage our training data... On AST we report BLEU scores."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "WMT23",
          "justification": "This dataset is used for evaluating multilingual translation capabilities.",
          "quote": "For our multilingual evaluations we use a multilingual math reasoning (MGSM; Shi et al., 2023a) benchmark and a machine translation benchmark (WMT23; Kocmi et al., 2023) which was constructed after the model’s training data cut-off hence minimizing test set leakage risks."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "EgoSchema",
          "justification": "This dataset is used for evaluating video understanding capabilities.",
          "quote": "The publicly available question answering benchmark with the longest videos is EgoSchema (Mangalam et al., 2023)... we introduce a new benchmark, 1H-VideoQA, composed of 125 five-way multiple-choice questions over public videos 40-105 minutes long."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "COVOST-2",
          "justification": "This dataset is used for evaluating speech-to-text translation capabilities.",
          "quote": "On CoVoST-2 we evaluate on translating speech in 20 languages into English, reporting on the subset of languages that were seen by the model during pre-training."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "BigBench",
          "justification": "This dataset is used to evaluate reasoning capabilities.",
          "quote": "On reasoning tasks, 1.5 Pro outperforms 1.0 Pro by a large margin and shows a comparable performance to 1.0 Ultra, slightly underperforming on DROP and slightly outperforming on BBH."
        },
        "role": "Used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "The training of Gemini 1.5 Pro was done using JAX.",
          "quote": "Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021)."
        },
        "role": "Used"
      },
      {
        "name": {
          "value": "SymPy",
          "justification": "The SymPy library is used for solving mathematical problems and integrating into the evaluation of the model.",
          "quote": "We present a simple example of MATH Intermediate Algebra problem where the solution involved SymPy. Here is the Python code to solve the problem: ... import sympy as sp"
        },
        "role": "Used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 2902,
    "prompt_tokens": 52052,
    "total_tokens": 54954
  }
}