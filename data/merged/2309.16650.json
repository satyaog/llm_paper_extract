{
  "title": {
    "value": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
    "justification": "The title precisely describes the primary contribution of the paper, which is the ConceptGraphs method.",
    "quote": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"
  },
  "description": "The paper proposes ConceptGraphs, a novel method to construct open-vocabulary and object-centric 3D scene graphs for robot perception and planning. It leverages 2D foundation models and fuses their outputs into 3D by multiview association to create a structured and semantically rich representation. The method allows robots to understand complex scenes, perform task planning, and handle abstract language queries without requiring large 3D datasets or model finetuning.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper includes experiments and real-world trials with robots to validate the proposed method, which classifies it as an empirical study.",
    "quote": "We implement and demonstrate ConceptGraphs on a number of real-world robotics tasks across wheeled and legged mobile robot platforms."
  },
  "research_field": {
    "value": "Robotics",
    "justification": "",
    "quote": "For robots to perform a wide variety of tasks, they require a 3D representation of the world that is semantically rich, yet compact and efficient for task-driven perception and planning.... In this work, we propose ConceptGraphs, an open-vocabulary graph-structured represen- tation for 3D scenes."
  },
  "sub_research_field": {
    "value": "Scene Understanding [[Scene Planning]]",
    "justification": "The paper focuses on 3D scene graphs which are an advanced representation for understanding the spatial relationships and semantic information in 3D scenes.",
    "quote": "ConceptGraphs focuses on the construction of the open- vocabulary 3D scene graphs for scene understanding and planning."
  },
  "models": [
    {
      "name": {
        "value": "CLIPSeg",
        "justification": "The model extracts visual features for the segmented regions.",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "CLIP is used for extracting visual features and embeddings within the ConceptGraphs framework.",
        "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
      },
      "mode": "finetuned"
    },
    {
      "name": {
        "value": "LSeg",
        "justification": "",
        "quote": "Language-driven semantic segmentation"
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "mode": "finetuned"
    },
    {
      "name": {
        "value": "OpenSeg",
        "justification": "",
        "quote": "Scaling open-vocabulary image segmentation with image-level labels"
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "mode": "finetuned"
    },
    {
      "name": {
        "value": "MaskCLIP",
        "justification": "The model extracts visual features for the segmented regions.",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "CLIP is used for extracting visual features and embeddings within the ConceptGraphs framework.",
        "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i = Embed(Itrgb , mt,i )."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "Mask2former",
        "justification": "",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "",
        "quote": "TABLE II: Open-vocabulary semantic segmentation on the Replica [56] dataset. Privileged methods specifically finetune the pretrained models for semantic segmentation. Zero-shot approaches do not need any finetuning and are evaluated off the shelf."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "ConceptGraphs",
        "justification": "This is the main model proposed and named in the paper.",
        "quote": "We propose ConceptGraphs, an open-vocabulary and object-centric 3D representation for robot perception and planning."
      },
      "role": "Contributed",
      "type": {
        "value": "Graph Neural Network",
        "justification": "",
        "quote": "ConceptGraphs constructs a map, a 3D scene graph."
      },
      "mode": "Trained"
    },
    {
      "name": {
        "value": "DINO",
        "justification": "DINO is used alongside CLIP for obtaining visual descriptors.",
        "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i =Embed(Itrgb , mt,i )."
      },
      "role": "Used",
      "type": {
        "value": "Visual Feature Extractor",
        "justification": "DINO is employed for extracting visual features from images.",
        "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i =Embed(Itrgb , mt,i )."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "GPT-4",
        "justification": "GPT-4 is used as the language model for inferring relationships and processing language queries.",
        "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
      },
      "role": "used",
      "type": {
        "value": "Large Language Model (LLM)",
        "justification": "GPT-4 is utilized for processing language-based tasks and queries within the ConceptGraphs framework.",
        "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "Grounding DINO",
        "justification": "The model is used in a variant of the ConceptGraphs system for open-vocabulary detection.",
        "quote": "An open-vocabulary 2D detector (Grounding DINO [34]) to obtain object bounding boxes."
      },
      "role": "used",
      "type": {
        "value": "Detection Model",
        "justification": "Grounding DINO is referenced but not the main focus model used for detection in ConceptGraphs.",
        "quote": "An open-vocabulary 2D detector (Grounding DINO [34]) to obtain object bounding boxes."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "LLaVA",
        "justification": "LLaVA is used for generating object captions from visual data.",
        "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
      },
      "role": "used",
      "type": {
        "value": "Vision-Language Model",
        "justification": "LLaVA is employed to interpret visual data and generate captions for identified objects.",
        "quote": "We use LLaVA [55] as the vision-language model LVLM and GPT-4 [32] for our LLM."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "SegmentAnything (SAM)",
        "justification": "The paper uses Segment Anything for segmentation tasks.",
        "quote": "Our experiments use SegmentAnything (SAM) [33] as the segmentation model Seg(·)"
      },
      "role": "Used",
      "type": {
        "value": "Segmentation Model",
        "justification": "SAM is used for segmentation tasks within the ConceptGraphs framework.",
        "quote": "Our experiments use SegmentAnything (SAM) [33] as the segmentation model Seg(·)"
      },
      "mode": "Inference"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "AI2Thor",
        "justification": "AI2Thor is used for simulation experiments.",
        "quote": "We showcase this with a … localization and remapping task in the AI2Thor [63], [64] simulation environment."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "Replica",
        "justification": "The paper uses Replica data scenes for evaluating scene graph accuracy.",
        "quote": "We evaluate on the Replica dataset [56] and a real- world scan of the REAL Lab, where we staged a number of items including clothes, tools, and toys."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "REAL Lab",
        "justification": "The paper uses Replica data scenes for evaluating scene graph accuracy.",
        "quote": "We evaluate on the Replica dataset [56] and a real- world scan of the REAL Lab, where we staged a number of items including clothes, tools, and toys."
      },
      "role": "Used"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Open3d SLAM",
        "justification": "Used for building initial pointcloud maps for the Jackal robot navigation.",
        "quote": "We begin by building a pointcloud of the REAL Lab using the onboard VLP-16 and Open3d SLAM [68]."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "RTAB-Map",
        "justification": "Utilized for obtaining camera poses and the scene point cloud in experiments.",
        "quote": "We then stage two separate scenes with different objects: one for object search and another for traversability estimation. In both cases, we map the scene with an Azure Kinect Camera and rely on RTAB-Map [69] to obtain camera poses and the scene point cloud."
      },
      "role": "used"
    }
  ]
}