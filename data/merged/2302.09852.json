{
  "title": {
    "value": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
    "justification": "Extracted from the title of the paper.",
    "quote": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"
  },
  "description": "The paper presents a novel unsupervised method for Out-of-Distribution (OOD) detection for textual data by aggregating layer-wise anomaly scores from a model's encoder. The authors demonstrate that the last layer's representation is not always the most effective for OOD detection and propose an automatic aggregation method to leverage all hidden layers without requiring access to OOD samples.",
  "type": {
    "value": "empirical study",
    "justification": "The paper involves conducting experiments to evaluate the performance of their proposed method on a new benchmark dataset and comparing it with existing methods.",
    "quote": "We conduct extensive experiments on our newly proposed benchmark: We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
  },
  "primary_research_field": {
    "value": "Out-of-distribution",
    "justification": "",
    "quote": ""
  },
  "sub_research_fields": [
    {
      "value": "Natural Language Processing",
      "justification": "The paper specifically addresses OOD detection in textual data, which falls under the sub-field of Natural Language Processing (NLP).",
      "quote": "Distinguishing OOD samples (OUT) from in-distribution (IN) samples is a challenge when working on complex data structures (e.g., text or image) due to their high dimensionality."
    }
  ],
  "models": [
    {
      "name": {
        "value": "BERT",
        "justification": "The experiments involve fine-tuning BERT models for various benchmark tests.",
        "quote": "We train classifiers based on ... BERT (Devlin et al. 2018) (base, large and multilingual versions) fine-tuned on each task."
      },
      "caracteristics": [
        {
          "value": "Transformer-based Model",
          "justification": "BERT is a well-known transformer-based model for Natural Language Processing tasks.",
          "quote": "We train classifiers based on ... BERT (Devlin et al. 2018) ..."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "DISTILBERT",
        "justification": "The paper mentions using DISTILBERT models for their experiments.",
        "quote": "We train classifiers based on ... DISTILBERT (Sanh et al. 2019) ..."
      },
      "caracteristics": [
        {
          "value": "Transformer-based Model",
          "justification": "DistilBERT is a more efficient, smaller version of BERT, and it follows the transformer-based architecture.",
          "quote": "We train classifiers based on ... DISTILBERT (Sanh et al. 2019) ..."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "RoBERTa",
        "justification": "RoBERTa models are used in the experiments as per the detailed mention in the paper.",
        "quote": "We train classifiers based on ... RoBERTa (Liu et al. 2019) ..."
      },
      "caracteristics": [
        {
          "value": "Transformer-based Model",
          "justification": "RoBERTa is an optimized version of BERT, and it is based on transformer architecture.",
          "quote": "We train classifiers based on ... RoBERTa (Liu et al. 2019) ..."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "20 Newsgroups",
        "justification": "The 20 Newsgroups dataset is mentioned as one of the datasets that the proposed method is evaluated on.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "IMDB",
        "justification": "IMDB dataset is used for evaluation in the paper.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MILTOOD-C",
        "justification": "MILTOOD-C is the new dataset proposed in the paper for multilingual textual OOD detection.",
        "quote": "We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
      },
      "role": "contributed",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "SST2",
        "justification": "SST2 is among the datasets used for text classification tasks for evaluation of the proposed method.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "SefFit/emotion",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "banking77",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "AmazonScience/massive",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "glue",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "super-glue",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": []
}