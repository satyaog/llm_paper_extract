{
  "title": {
    "value": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
    "justification": "The title is clearly mentioned at the beginning and in the header of the paper.",
    "quote": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"
  },
  "description": "The paper presents XTREME-UP, a benchmark designed to evaluate the capabilities of language models on under-represented languages (ULs) using scarce data. It focuses on user-centric tasks prevalent among high-resource languages. The benchmark comprises 88 languages and nine key technologies, including OCR, ASR, MT, autocomplete, semantic parsing, and transliteration. The paper provides baseline results and recommends the development of more inclusive multilingual NLP technologies.\n",
  "type": {
    "value": "empirical",
    "justification": "The study provides experimental results using the X TREME -U P benchmark to evaluate different language models.",
    "quote": "We evaluate commonly used models on the benchmark."
  },
  "primary_research_field": {
    "value": "Natural Language Processing",
    "justification": "The study is centered on developing and evaluating NLP models for multilingual tasks.",
    "quote": "We propose X TREME -U P, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks."
  },
  "sub_research_fields": [
    {
      "value": "Multilingual",
      "justification": "The focus is on under-represented languages and tasks that can be tackled using textual data, specifically for languages with scarce data.",
      "quote": "XTREME-UP focuses on under-represented languages and user-centric tasks, creating new data for under-represented tasks and languages."
    },
    {
      "value": "Low-Resource",
      "justification": "",
      "quote": ""
    },
    {
      "value": "Language Processing",
      "justification": "",
      "quote": ""
    }
  ],
  "models": [
    {
      "name": {
        "value": "ByT5",
        "justification": "ByT5 is discussed and evaluated within the XTREME-UP benchmark, particularly emphasizing the byte-based approaches.",
        "quote": "ByT5-base (Xue et al., 2022), a byte-based multilingual encoder-decoder model."
      },
      "caracteristics": [
        {
          "value": "Byte-Level Multilingual Model",
          "justification": "ByT5 is designed to operate on byte-level inputs, making it suitable for handling various languages at the byte level.",
          "quote": "ByT5-base (Xue et al., 2022)... a byte-based multilingual encoder-decoder model."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Flan-PaLM",
        "justification": "Flan-PaLM is highlighted as an in-context learning model evaluated in the XTREME-UP benchmark.",
        "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
      },
      "caracteristics": [
        {
          "value": "Instruction-Tuned Language Model",
          "justification": "Flan-PaLM is an instruction-tuned large language model, making it suitable for in-context learning tasks.",
          "quote": "Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "mT5",
        "justification": "mT5 is explicitly mentioned and evaluated across various tasks within the XTREME-UP benchmark.",
        "quote": "The data for each language is sub-sampled to emulate data sizes that can be realistically annotated within a reasonable time frame... We evaluate mT5-base (Xue et al., 2021)."
      },
      "caracteristics": [
        {
          "value": "Multilingual Transformer",
          "justification": "mT5 is a multilingual transformer-based model specialized for text-to-text tasks.",
          "quote": "mT5-base (Xue et al., 2021) and a subword-based multilingual encoder-decoder model."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "Dakshina",
        "justification": "Used for the transliteration task in the benchmark.",
        "quote": "Most of the data for the task comes from the romanized full-string subset of the Dakshina dataset (Roark et al., 2020), in which 10,000 Wikipedia sentences written in the native scripts of the 12 languages were human-romanized by native speakers, resulting in parallel sentences in the native and Latin scripts."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "FLEURS",
        "justification": "FLEURS is used for evaluating ASR tasks in the benchmark.",
        "quote": "We employ the FLEURS dataset (Conneau et al., 2023) consisting of recordings in 102 languages for sentences from FLORES-101 (Goyal et al., 2022), which were translated from English Wikipedia to 101 languages."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "FLORES-101",
        "justification": "Adapted for machine translation tasks in the benchmark.",
        "quote": "The dataset is adapted from FLORES-101 (Goyal et al., 2022), repurposing half of the datasetâ€™s original development set as a training set."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MasakhaNER",
        "justification": "The dataset provides data for the named entity recognition task in the benchmark.",
        "quote": "The dataset contains processed data from MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al., 2022)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "TyDi QA",
        "justification": "Used for evaluating question answering tasks in the X TREME -U P benchmark.",
        "quote": "In the in-language QA task, both the question and passage are in the same language. In this task, original questions and passages are from the TyDi QA dataset (Clark et al., 2020)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Universal Dependencies",
        "justification": "This dataset was used to test models' predictive capabilities rather than their memorization capabilities.",
        "quote": "We process high-quality natural language data from Universal Dependencies (de Marneffe et al., 2021), which we deduplicate against mC4 (Xue et al., 2021), the most common multilingual pre-training corpus in order to test models predictive rather than memorization capabilities."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "T5X",
        "justification": "T5X was used to train models on the XTREME-UP benchmark.",
        "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "seqio",
        "justification": "seqio was used for training models on the benchmark.",
        "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}