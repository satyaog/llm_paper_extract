{
  "description": "The paper proposes the Ordered Memory architecture, inspired by Ordered Neurons. It introduces a new attention-based mechanism and a new Gated Recursive Cell (GRC). The Ordered Memory model is evaluated on tasks like logical inference and list operations, showing strong performance and the ability to interpret induced tree structures.",
  "title": {
    "value": "Ordered Memory",
    "justification": "Title of the paper as presented in the document",
    "quote": "Ordered Memory"
  },
  "type": {
    "value": "empirical",
    "justification": "The paper presents an architecture and evaluates it on several experiments with datasets.",
    "quote": "In this paper, we propose a novel architecture: Ordered Memory (OM), which includes a new memory updating mechanism and a new Gated Recursive Cell. We demonstrate that our method generalizes for synthetic tasks where the ability to parse is crucial to solving them."
  },
  "research_field": {
    "value": "Deep Learning",
    "justification": "The paper proposes a new neural network architecture and evaluates it on deep learning tasks.",
    "quote": "Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some time."
  },
  "sub_research_field": {
    "value": "Long-Term Memory [[Recurrent Neural Networks, Memory]]",
    "justification": "",
    "quote": ""
  },
  "models": [
    {
      "name": {
        "value": "ON-LSTM",
        "justification": "Mentioned as a model related to the proposed Ordered Memory architecture.",
        "quote": "Inspired by Ordered Neurons (Shen et al., 2018)"
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "ON-LSTM is a neural network model with ordered neurons.",
        "quote": "Inspired by Ordered Neurons (Shen et al., 2018)"
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "Ordered Memory",
        "justification": "It is the main contribution of the paper.",
        "quote": "In this paper, we propose the Ordered Memory architecture."
      },
      "role": "contributed",
      "type": {
        "value": "RNN",
        "justification": "Ordered Memory is a neural network model.",
        "quote": "Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some time."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "RL-SPINN",
        "justification": "Mentioned as a model related to the proposed Ordered Memory architecture.",
        "quote": "Yogatama et al. (2016) proposes RL-SPINN where the discrete stack operations are directly learned by reinforcement learning."
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "RL-SPINN is a type of neural network with reinforcement learning.",
        "quote": "Yogatama et al. (2016) proposes RL-SPINN where the discrete stack operations are directly learned by reinforcement learning."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "RRNet",
        "justification": "Mentioned as a model related to the proposed Ordered Memory architecture.",
        "quote": "The results for RRNet were taken from Jacob et al. (2018)."
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "RRNet is a type of neural network related to Ordered Memory.",
        "quote": "The results for RRNet were taken from Jacob et al. (2018)."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "Transformer",
        "justification": "Used for comparison in the experiments.",
        "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
      },
      "role": "used",
      "type": {
        "value": "Transformer",
        "justification": "Transformer is a type of neural network for handling sequential data.",
        "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "TreeLSTM",
        "justification": "Used for comparison in the experiments.",
        "quote": "The TreeCell is a recursive neural network based on the Gated Recursive Cell function proposed in section 3.2."
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "TreeLSTM is a type of neural network for handling tree structures.",
        "quote": "The TreeCell is a recursive neural network based on the Gated Recursive Cell function proposed in section 3.2."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "TreeCell",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "",
        "quote": ""
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "TreeRNN",
        "justification": "",
        "quote": ""
      },
      "role": "used",
      "type": {
        "value": "RNN",
        "justification": "",
        "quote": ""
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "Universal Transformer",
        "justification": "Used for comparison in the experiments.",
        "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
      },
      "role": "used",
      "type": {
        "value": "Transformer",
        "justification": "Universal Transformer is a type of neural network for handling sequential data.",
        "quote": "For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al. (2018)."
      },
      "mode": "trained"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "ListOps",
        "justification": "Used to evaluate the model.",
        "quote": "and the ListOps (Nangia and Bowman, 2018) task."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "SST-2",
        "justification": "A subset task from Stanford Sentiment Treebank used in the experiments.",
        "quote": "We also perform experiments on the Stanford Sentiment Treebank, in both binary classification and fine-grained settings (SST-2 & SST-5)"
      },
      "role": "used"
    },
    {
      "name": {
        "value": "SST-5",
        "justification": "A subset task from Stanford Sentiment Treebank used in the experiments.",
        "quote": "We also perform experiments on the Stanford Sentiment Treebank, in both binary classification and fine-grained settings (SST-2 & SST-5)"
      },
      "role": "used"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Evalb",
        "justification": "Used to evaluate parsing performance.",
        "quote": "We evaluate parsing performance using the F1 score3 . All parsing scores are given by Evalb"
      },
      "role": "used"
    }
  ]
}