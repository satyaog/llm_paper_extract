title:
  value: Ordered Memory
  justification: Title of the paper as presented in the document
  quote: Ordered Memory
description: The paper proposes the Ordered Memory architecture, inspired by Ordered Neurons. It introduces a new attention-based
  mechanism and a new Gated Recursive Cell (GRC). The Ordered Memory model is evaluated on tasks like logical inference and
  list operations, showing strong performance and the ability to interpret induced tree structures.
type:
  value: empirical
  justification: The paper presents an architecture and evaluates it on several experiments with datasets.
  quote: 'In this paper, we propose a novel architecture: Ordered Memory (OM), which includes a new memory updating mechanism
    and a new Gated Recursive Cell. We demonstrate that our method generalizes for synthetic tasks where the ability to parse
    is crucial to solving them.'
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper proposes a new neural network architecture and evaluates it on deep learning tasks.
    quote: Stack-augmented recurrent neural networks (RNNs) have been of interest to the deep learning community for some
      time.
  aliases: []
sub_research_fields:
- name:
    value: Long-Term Memory
    justification: ''
    quote: ''
  aliases: []
- name:
    value: Recurrent Neural Networks
    justification: ''
    quote: ''
  aliases: []
- name:
    value: Memory
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: ON-LSTM
    justification: Mentioned as a model related to the proposed Ordered Memory architecture.
    quote: Inspired by Ordered Neurons (Shen et al., 2018)
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Ordered Memory
    justification: It is the main contribution of the paper.
    quote: In this paper, we propose the Ordered Memory architecture.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: RL-SPINN
    justification: Mentioned as a model related to the proposed Ordered Memory architecture.
    quote: Yogatama et al. (2016) proposes RL-SPINN where the discrete stack operations are directly learned by reinforcement
      learning.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: RRNet
    justification: Mentioned as a model related to the proposed Ordered Memory architecture.
    quote: The results for RRNet were taken from Jacob et al. (2018).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Transformer
    justification: Used for comparison in the experiments.
    quote: For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al.
      (2018).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TreeLSTM
    justification: Used for comparison in the experiments.
    quote: The TreeCell is a recursive neural network based on the Gated Recursive Cell function proposed in section 3.2.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TreeCell
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TreeRNN
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Universal Transformer
    justification: Used for comparison in the experiments.
    quote: For the Transformer and Universal Transformer, we follow the entailment architecture introduced in Radford et al.
      (2018).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: ListOps
    justification: Used to evaluate the model.
    quote: and the ListOps (Nangia and Bowman, 2018) task.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SST-2
    justification: A subset task from Stanford Sentiment Treebank used in the experiments.
    quote: We also perform experiments on the Stanford Sentiment Treebank, in both binary classification and fine-grained
      settings (SST-2 & SST-5)
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SST-5
    justification: A subset task from Stanford Sentiment Treebank used in the experiments.
    quote: We also perform experiments on the Stanford Sentiment Treebank, in both binary classification and fine-grained
      settings (SST-2 & SST-5)
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: Evalb
    justification: Used to evaluate parsing performance.
    quote: We evaluate parsing performance using the F1 score3 . All parsing scores are given by Evalb
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
