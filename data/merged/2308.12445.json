{
  "description": "This paper proposes a novel self-healing approach for Deep Reinforcement Learning (DRL) systems called Dr. DRL, which integrates intentional forgetting mechanisms into Continual Learning (CL) to address issues like catastrophic forgetting and slow convergence in DRL systems. This approach selectively forgets minor behaviors in order to prioritize the DRL system's key problem-solving skills, thereby accelerating adaptation to environmental drifts and improving overall performance.",
  "title": {
    "value": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems",
    "justification": "The title is clearly stated at the beginning of the paper.",
    "quote": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"
  },
  "type": {
    "value": "empirical study",
    "justification": "The paper includes experimental evaluations of the proposed approach using different DRL algorithms and environments.",
    "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL (i.e., standard CL) to overcome its main issues... To demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments with different drifting intensities."
  },
  "research_field": {
    "value": "Deep reinforcement learning (DRL)",
    "justification": "",
    "quote": "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which of- ten occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment’s conditions shifts."
  },
  "sub_research_field": {
    "value": "Continual Learning (CL)",
    "justification": "",
    "quote": "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook. As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which of- ten occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach for adapting the DRL agent in response to the environment’s conditions shifts."
  },
  "models": [
    {
      "name": {
        "value": "Deep Q-Learning (DQN)",
        "justification": "The paper mentions the use of Deep Q-Learning (DQN) as one of the DRL algorithms used for evaluation.",
        "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
      },
      "role": "used",
      "type": {
        "value": "DRL",
        "justification": "",
        "quote": ""
      },
      "mode": "evaluation"
    },
    {
      "name": {
        "value": "Dr. DRL",
        "justification": "Dr. DRL is the main model proposed in the paper.",
        "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL systems that integrates a novel mechanism of intentional forgetting into vanilla CL (i.e., standard CL) to overcome its main issues."
      },
      "role": "contributed",
      "type": {
        "value": "DRL",
        "justification": "",
        "quote": ""
      },
      "mode": "training"
    },
    {
      "name": {
        "value": "Proximal Policy Optimization (PPO)",
        "justification": "The paper mentions the use of Proximal Policy Optimization (PPO) as one of the DRL algorithms used for evaluation.",
        "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
      },
      "role": "used",
      "type": {
        "value": "DRL",
        "justification": "",
        "quote": ""
      },
      "mode": "evaluation"
    },
    {
      "name": {
        "value": "Soft Actor-Critic (SAC)",
        "justification": "The paper mentions the use of Soft Actor-Critic (SAC) as one of the DRL algorithms used for evaluation.",
        "quote": "three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
      },
      "role": "used",
      "type": {
        "value": "DRL",
        "justification": "",
        "quote": ""
      },
      "mode": "evaluation"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "Acrobot",
        "justification": "The paper evaluates the proposed approach using the Acrobot environment.",
        "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments... CartePole [28], MountainCar [29], and Acrobot [30]"
      },
      "role": "used"
    },
    {
      "name": {
        "value": "CartPole",
        "justification": "The paper evaluates the proposed approach using the CartPole environment.",
        "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments... CartePole [28], MountainCar [29], and Acrobot [30]"
      },
      "role": "used"
    },
    {
      "name": {
        "value": "MountainCar",
        "justification": "The paper evaluates the proposed approach using the MountainCar environment.",
        "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments... CartePole [28], MountainCar [29], and Acrobot [30]"
      },
      "role": "used"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "TensorFlow",
        "justification": "The paper states that the implementation supports TensorFlow and uses it for the empirical evaluation.",
        "quote": "We implemented our approach as an open-source tool using Python 3.7 [49] and it supports Tensorflow (version 2.4.4) [50]."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "Gym library",
        "justification": "",
        "quote": "We evaluated environments from the Gym library (version 0.23.1) [27]"
      },
      "role": "used"
    }
  ]
}