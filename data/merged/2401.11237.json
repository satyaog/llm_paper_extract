{
  "description": "This paper examines the capacity of certain reinforcement learning (RL) algorithms to stitch together pieces of experience to solve tasks not seen during training, and investigates whether supervised learning (SL)-based RL methods possess this stitching property. It introduces new datasets to test this and proposes temporal data augmentation to improve generalization in SL-based RL methods.",
  "title": {
    "value": "Closing the Gap Between TD Learning and Supervised Learning – A Generalization Point of View",
    "justification": "This title encapsulates the core comparison between Temporal Difference (TD) learning in RL and Supervised Learning approaches, with a focus on generalization.",
    "quote": "Closing the Gap Between TD Learning and Supervised Learning – A Generalisation Point of View"
  },
  "type": {
    "value": "Empirical study",
    "justification": "The paper includes both theoretical analysis and empirical results to support its claims.",
    "quote": "Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalisation reveals why we should not expect SL-based RL methods to perform stitching, even in the limit of large datasets and models. Based on this analysis, we construct new datasets to explicitly test for this property, revealing that SL-based methods lack this stitching property and hence fail to perform combinatorial generalization."
  },
  "research_field": {
    "value": "Reinforcement Learning",
    "justification": "The research primarily focuses on RL, particularly the differences between TD learning and SL in the context of RL.",
    "quote": "Our work shows that combinatorial generalisation is also required to solve tasks in the context of RL."
  },
  "sub_research_field": {
    "value": "Goal-conditioned [[Reinforcement Learning]]",
    "justification": "The paper discusses the importance of goal-conditioned policies and their ability to generalize to new state-goal pairs during testing.",
    "quote": "We will study the problem of goal-conditioned RL in a controlled Markov process with states s ∈ S and actions a ∈ A."
  },
  "models": [
    {
      "name": {
        "value": "DDPG",
        "justification": "DDPG is cited as another RL method that has the stitching property.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning",
        "justification": "DDPG applies deep learning to deterministic policy gradients in RL settings.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "DQN",
        "justification": "DQN is mentioned as an RL algorithm with the stitching property.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning",
        "justification": "DQN is an RL algorithm known for its use of Q-learning for dynamic programming.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "Decision Transformer (DT)",
        "justification": "The model is explicitly mentioned and used in experiments to test combinatorial generalization capabilities.",
        "quote": "DT [2] shows experiments where their SL-based method performs stitching."
      },
      "role": "used",
      "type": {
        "value": "Transformer-based model",
        "justification": "The model utilizes a transformer architecture.",
        "quote": "DT [2] treats RL as a sequential SL problem and uses the transformer architecture as a policy."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "IQL",
        "justification": "IQL is mentioned as part of RL algorithms showing the stitching property.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning",
        "justification": "IQL is another RL algorithm that utilizes implicit Q-learning methods.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "RvS (Return-Conditioned Supervised Learning)",
        "justification": "The model is mentioned as being used for empirical evaluation and comparison.",
        "quote": "RvS [3] shows that a simple SL-based algorithm can surpass the performance of TD algorithms."
      },
      "role": "used",
      "type": {
        "value": "Supervised Learning model",
        "justification": "The model is a simple SL-based algorithm applied to RL.",
        "quote": "RvS [3] shows that a simple SL-based algorithm can surpass the performance of TD algorithms."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "TD3",
        "justification": "TD3 is included as an example of an RL algorithm benefiting from the stitching property.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning",
        "justification": "TD3 improves upon DDPG by addressing function approximation errors.",
        "quote": "The stitching property [5] is common among RL algorithms that perform dynamic programming (e.g., DQN [6], DDPG [7], TD3 [8], IQL [9])."
      },
      "mode": "inference"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "D4RL datasets",
        "justification": "The datasets are used for offline RL experiments in the paper.",
        "quote": "Our experiments reveal a subtle consideration with the common D4RL datasets [15]: while these datasets are purported to test for exactly this sort of combinatorial generalization, data analysis reveals that “unseen” (state, goal) pairs do actually appear in the dataset."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "Offline Ant Maze",
        "justification": "The Offline Ant Maze dataset is used in the context of RL to evaluate the stitching property.",
        "quote": "In our experiments, we collect new offline datasets that precisely test for stitching (see Fig. 3 and Fig. 12 for visualisation)."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "Miniworld",
        "justification": "",
        "quote": "We visualise our new image based and partially observable environment created using Miniworld"
      },
      "role": "used"
    },
    {
      "name": {
        "value": "new variant of D4RL datasets",
        "justification": "The authors created new variants of the D4RL datasets to better test for combinatorial generalization.",
        "quote": "Thus, our experiments are run on a new variant of these datasets that we constructed for this paper to explicitly test for combinatorial generalization."
      },
      "role": "contributed"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "scikit-learn",
        "justification": "The library is used for clustering states in the temporal data augmentation process.",
        "quote": "We use the k-means algorithm from scikit-learn [62] with the default parameters to group states together."
      },
      "role": "used"
    }
  ]
}