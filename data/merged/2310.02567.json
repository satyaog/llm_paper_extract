{
  "title": {
    "value": "Improving Automatic VQA Evaluation Using Large Language Models",
    "justification": "The title directly reflects the main focus and contribution of the paper.",
    "quote": "Improving Automatic VQA Evaluation Using Large Language Models"
  },
  "description": "This paper proposes a new metric for automatic Visual Question Answering (VQA) evaluation named LAVE (LLM-Assisted VQA Evaluation), which leverages instruction-tuned large language models (LLMs) to better correlate with human judgment compared to existing metrics.",
  "type": {
    "value": "Empirical study",
    "justification": "The paper conducts experiments analyzing the effectiveness of the proposed LAVE metric by collecting human judgments and comparing them to existing metrics.",
    "quote": "We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
  },
  "primary_research_field": {
    "value": "Computer Vision",
    "justification": "The paper deals with Visual Question Answering (VQA), a key benchmark in the field of Computer Vision.",
    "quote": "Visual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress of multimodal vision-language systems."
  },
  "sub_research_fields": [
    {
      "value": "Visual Question Answering",
      "justification": "The study focuses specifically on improving the evaluation of Visual Question Answering systems.",
      "quote": "We propose a novel automatic VQA evaluation metric, LAVE (LLM-Assisted VQA Evaluation), which leverages the in-context learning capabilities of instruction-tuned LLMs."
    }
  ],
  "models": [
    {
      "name": {
        "value": "BLIP-2",
        "justification": "BLIP is used as another representative VQA model for finetuning and evaluation.",
        "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA) and VG-QA (BLIPVG), which represents the finetuning-OOD paradigm."
      },
      "caracteristics": [
        {
          "value": "VQA Model",
          "justification": "BLIP is identified as a Vision-Language model used for Visual Question Answering.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ), which represents the finetuning-OOD paradigm."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PromptCap",
        "justification": "PromptCap is used as a state-of-the-art VQA model in evaluations.",
        "quote": "We focus on BLIP-2 and PromptCap since their generation is most open-ended."
      },
      "caracteristics": [
        {
          "value": "VQA Model",
          "justification": "PromptCap is identified as another Vision-Language model used for the task of Visual Question Answering.",
          "quote": "We focus on BLIP-2 and PromptCap since their generation is most open-ended."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "BLIP_VG",
        "justification": "<missing>",
        "quote": "<missing>"
      },
      "caracteristics": [
        {
          "value": "VQA Model",
          "justification": "<missing>",
          "quote": "<missing>"
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "BLIP_VQA",
        "justification": "<missing>",
        "quote": "<missing>"
      },
      "caracteristics": [
        {
          "value": "VQA Model",
          "justification": "<missing>",
          "quote": "<missing>"
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "OK-VQA",
        "justification": "The OK-VQA dataset is mentioned as one of the benchmarks for evaluating the proposed metric, LAVE.",
        "quote": "We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna et al. 2017) and OK-VQA (Marino et al. 2019)."
      },
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "VG-QA",
        "justification": "The VG-QA dataset is used for evaluating the performance of LAVE.",
        "quote": "We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna et al. 2017) and OK-VQA (Marino et al. 2019)."
      },
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "VQAv2",
        "justification": "VQAv2 is one of the VQA datasets used for evaluation.",
        "quote": "Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "HuggingFace Transformers",
        "justification": "The paper mentions using the HuggingFace Transformers library for implementation purposes.",
        "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020) implementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1."
      },
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}