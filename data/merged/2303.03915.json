{
  "description": "This paper details the creation and analysis of the BigScience ROOTS corpus, a comprehensive multilingual dataset consisting of 1.6TB of text spanning 59 languages. It documents the data curation efforts, including the roles of various working groups, the preprocessing and filtering steps, as well as the ethical considerations behind the project. The dataset was used to train the 176-billion-parameter BLOOM language model.",
  "title": {
    "value": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
    "justification": "This is the exact title of the research paper provided by the user.",
    "quote": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
  },
  "type": {
    "value": "Empirical Study",
    "justification": "The paper focuses on the empirical creation, curation, and analysis of a large multilingual dataset. It includes specific methods for data cleaning, filtering, and deduplication, and provides statistical analyses of the dataset.",
    "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
  },
  "research_field": {
    "value": "Natural Language Processing",
    "justification": "The research focuses on creating a multilingual corpus for training large language models, which is a core area of Natural Language Processing.",
    "quote": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings."
  },
  "sub_research_field": {
    "value": "Multilingual [[Large Language Models]]",
    "justification": "The sub-field focuses on multilingual datasets and their use in training large-scale language models, aligning with the paper's goal of creating and analyzing a multilingual dataset for the BLOOM language model.",
    "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Opencollaboration Open-access Multilingual language model."
  },
  "models": [
    {
      "name": {
        "value": "BLOOM",
        "justification": "The BLOOM model is trained using the ROOTS corpus, making it the main model discussed in the paper.",
        "quote": "that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model."
      },
      "role": "referenced",
      "type": {
        "value": "Large Language Model",
        "justification": "The paper specifies that the BLOOM model is a large multilingual language model designed for various NLP tasks.",
        "quote": "the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model."
      },
      "mode": "Trained"
    },
    {
      "name": {
        "value": "GPT-3",
        "justification": "The paper references GPT-3 as a comparative model to the objectives they had with BLOOM.",
        "quote": "One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in scale to GPT-3 (Brown et al., 2020)."
      },
      "role": "Referenced",
      "type": {
        "value": "Transformer-based Language Model",
        "justification": "GPT-3 is well-known as a transformer-based language model.",
        "quote": "One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in scale to GPT-3 (Brown et al., 2020)."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "T5",
        "justification": "The paper mentions T5 in the context of discussing the datasets used in training language models like T5.",
        "quote": "Opt: Open Pre-trained Transformer Language Models, mC4 (Raffel et al., 2020), which have powered the T5 family of models."
      },
      "role": "referenced",
      "type": {
        "value": "Transformer-based Language Model",
        "justification": "T5 is a well-known transformer-based language model.",
        "quote": "mC4 (Raffel et al., 2020), which have powered the T5 family of models."
      },
      "mode": "trained"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "CC100",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling."
      },
      "role": "Referenced"
    },
    {
      "name": {
        "value": "ROOTS",
        "justification": "The ROOTS dataset is the main contribution of the paper, meticulously created and processed as a large-scale multilingual text corpus.",
        "quote": "assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
      },
      "role": "Contributed"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Indic NLP Library",
        "justification": "Indic NLP Library is used for sentence tokenization for multiple Indian languages as part of the data processing pipeline.",
        "quote": "For Bengalic, Gujarati, Hindi, Kannada, Malayalam, Marathi, Punjabi, Tamil, and Telugu, we use the Indic NLP library tokenizer (Kunchukuttan, 2020)."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "KenLM",
        "justification": "KenLM is used to train 5-gram models after tokenization for OSCAR data filtering.",
        "quote": "KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was extracted from OSCAR."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "NLTK",
        "justification": "NLTK is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
        "quote": "For English, French, Portuguese, and Spanish, we use the NLTK tokenizer (Bird et al., 2009)."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "SentencePiece",
        "justification": "SentencePiece is used to train unigram tokenizers for multiple languages for OSCAR data filtering.",
        "quote": "Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018)."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "Stanza",
        "justification": "Stanza is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
        "quote": "For Arabic, Catalan, Basque, Indonesian, and Chinese (both simplified and traditional), we use the Stanza tokenizer (Qi et al., 2020)."
      },
      "role": "Used"
    },
    {
      "name": {
        "value": "Underthesea",
        "justification": "Underthesea is used for sentence tokenization for the Vietnamese language as part of the data processing pipeline.",
        "quote": "For Vietnamese, we use the Underthesea tokenizer."
      },
      "role": "Used"
    }
  ]
}