{
  "description": "The paper presents an offline reinforcement learning algorithm, ORBIT (Offline RL-Bootstrapped InTerface), which optimizes human-machine interfaces for sequential decision-making tasks like robotic teleoperation. The method combines offline pre-training with online fine-tuning to improve the interface's performance, particularly in noisy and high-dimensional command settings such as eye-gaze-based control.",
  "title": {
    "value": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning",
    "justification": "This is the title as given in the document.",
    "quote": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"
  },
  "type": {
    "value": "empirical",
    "justification": "The research includes user studies with participants performing tasks using the proposed ORBIT method and compares the performance with baseline methods.",
    "quote": "To evaluate ORBIT’s ability to learn an effective interface from real user data, we conduct a user study with 12 participants who perform a simulated navigation task by using their eye gaze to modulate a 128dimensional command signal from their webcam (illustrated in Fig. 3 and 5)."
  },
  "research_field": {
    "value": "Reinforcement Learning",
    "justification": "The paper introduces a reinforcement learning algorithm and discusses its application and evaluation in sequential decision-making tasks.",
    "quote": "Recent work on human-in-the-loop reinforcement learning ... lifts these assumptions, but still assumes access to either a task-agnostic reward function, task distribution, or prior interface."
  },
  "sub_research_field": {
    "value": "Human-in-the-loop [[Reinforcement Learning]]",
    "justification": "The research focuses on interaction with users, especially using their feedback and commands to improve the learning algorithm.",
    "quote": "To address the high cost of user data, we pre-train value functions on a large offline dataset, and distill them into a policy (i.e., an interface) that infers the user’s desired action from the user’s command signal."
  },
  "models": [
    {
      "name": {
        "value": "ORBIT",
        "justification": "ORBIT is the main model and contribution described in the paper.",
        "quote": "We call our method the Offline RL-Bootstrapped InTerface (ORBIT)."
      },
      "role": "contributed",
      "type": {
        "value": "Reinforcement Learning model",
        "justification": "ORBIT is described as an offline reinforcement learning algorithm designed to optimize human-machine interfaces.",
        "quote": "We propose an offline RL algorithm for interface optimization that can learn from both an observational dataset of the user attempting to perform their desired tasks using some unknown existing default interface, as well as online data collected using our learned interface."
      },
      "mode": "trained"
    },
    {
      "name": {
        "value": "COACH",
        "justification": "",
        "quote": "Prior work on COACH [42], [43], TAMER [44], [45], and preference learning [46]–[50] trains RL agents from human feedback."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning Model",
        "justification": "",
        "quote": ""
      },
      "mode": "inference"
    },
    {
      "name": {
        "value": "TAMER",
        "justification": "",
        "quote": "Prior work on COACH [42], [43], TAMER [44], [45], and preference learning [46]–[50] trains RL agents from human feedback."
      },
      "role": "referenced",
      "type": {
        "value": "Reinforcement Learning Model",
        "justification": "",
        "quote": ""
      },
      "mode": "inference"
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "Expert User Data on Navigation Task",
        "justification": "The paper describes collecting a large dataset from an expert user performing navigation tasks using a directional interface.",
        "quote": "Hence, we collect a large offline dataset of 1200 episodes of an expert user (the first author) performing tasks using their eye gaze and the default directional interface."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "Lunar Lander Game Data",
        "justification": "The paper describes using simulated user commands on the Lunar Lander game to evaluate their model.",
        "quote": "We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands..."
      },
      "role": "used"
    },
    {
      "name": {
        "value": "Simulated User Data on Navigation Task",
        "justification": "The paper presents results from navigation tasks using simulated noisy user commands, to validate their algorithm's components.",
        "quote": "To perform ablations of ORBIT at a scale that would be impractical for a user study, we simulate noisy, high dimensional user commands xt for the navigation task..."
      },
      "role": "used"
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Stable Baselines3",
        "justification": "The paper mentions using the Stable Baselines3 library for simulations, such as training agents for the noisy user commands.",
        "quote": "We train an agent to act as the simulated user using the proximal policy optimization algorithm (PPO) [58] and the default hyperparameter values from the Stable Baselines3 library [59]."
      },
      "role": "used"
    }
  ]
}