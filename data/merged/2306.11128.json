{
  "title": {
    "value": "CAMMARL : Conformal Action Modeling in Multi Agent Reinforcement Learning",
    "justification": "The title is clearly stated at the top of the text.",
    "quote": "CAMMARL : Conformal Action Modeling in Multi Agent Reinforcement Learning"
  },
  "description": "This research proposes a novel multi-agent reinforcement learning (MARL) algorithm named CAMMARL. The main idea is to model the actions of other agents as conformal prediction sets, which provide high-probability guarantees of containing the true actions. The algorithm aims to optimize an autonomous agentâ€™s decision-making capabilities in multi-agent environments. The performance of CAMMARL is demonstrated through various experiments in cooperative multi-agent tasks.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper includes several experiments to demonstrate the performance of the proposed CAMMARL algorithm in different multi-agent environments.",
    "quote": "Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities of an autonomous agent in MARL by modeling conformal prediction sets over the behavior of other agents."
  },
  "primary_research_field": {
    "value": "Deep Learning",
    "justification": "The focus is on a novel algorithm for multi-agent reinforcement learning, a subfield of machine learning which is inherently a part of deep learning.",
    "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL, which involves modeling the actions of other agents in different situations in the form of confident sets."
  },
  "sub_research_fields": [
    {
      "value": "Multi-Agent",
      "justification": "The core contribution of the paper is an algorithm designed for multi-agent reinforcement learning scenarios.",
      "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability."
    },
    {
      "value": "Reinforcement Learning",
      "justification": "",
      "quote": ""
    }
  ],
  "models": [
    {
      "name": {
        "value": "CAMMARL",
        "justification": "This is the main model proposed by the authors.",
        "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability."
      },
      "caracteristics": [
        {
          "value": "Multi-Agent Reinforcement Learning Model",
          "justification": "The model is specifically tailored for multi-agent reinforcement learning tasks.",
          "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability."
        }
      ],
      "is_contributed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PPO",
        "justification": "",
        "quote": "We use proximal policy optimization (PPO) (Schulman et al., 2017) to update the decision-making policy for both the RL agents"
      },
      "caracteristics": [
        {
          "value": "Reinforcement Learning Algorithm",
          "justification": "",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MLP",
        "justification": "",
        "quote": "For the individual actor and critic networks, we used 2 fully-connected multi-layer perceptron (MLP) layers."
      },
      "caracteristics": [
        {
          "value": "Neural Network",
          "justification": "",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "Cooperative Navigation",
        "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
        "quote": "We focus on four cooperative multi-agent environments: Cooperative Navigation."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Google Football",
        "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
        "quote": "Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper in a game of football."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Level-based Foraging",
        "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
        "quote": "We focus on four cooperative multi-agent environments: Level-based Foraging."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Pressure Plate",
        "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
        "quote": "Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper in a game of football."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": []
}